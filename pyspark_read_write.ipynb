{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('read_write_Files').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some sample data and convert to DF\n",
    "\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from faker import Faker\n",
    "fake = Faker()\n",
    "\n",
    "def generate_sample_data(num_entries=5):\n",
    "    names = [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eva\"]\n",
    "    departments = [\"HR\", \"IT\", \"Finance\", \"Marketing\", \"Operations\"]\n",
    "\n",
    "    current_year = datetime.now().year\n",
    "    date_of_births = [datetime(current_year - random.randint(10, 50), random.randint(1, 12), random.randint(1, 28)) for _ in range(num_entries)]\n",
    "\n",
    "    sample_data = []\n",
    "\n",
    "    for _ in range(num_entries):\n",
    "        name = fake.first_name()\n",
    "        age = current_year - date_of_births[_].year\n",
    "        dept = random.choice(departments)\n",
    "        salary = round(random.uniform(40000, 120000) , 1)\n",
    "        dob = date_of_births[_].strftime(\"%Y-%m-%d\")\n",
    "        married = random.choice([True, False])\n",
    "        gender = random.choice([\"M\", \"F\", None])\n",
    "        # array of languages spoken\n",
    "        langs = random.sample([\"English\", \"Spanish\", \"French\", \"German\", \"Chinese\"], random.randint(2, 3))\n",
    "        # contact information (dictionary)\n",
    "        contact = {'Phone': fake.phone_number(), 'Email': fake.email()}\n",
    "        # skills (dictionary)\n",
    "        coms = {'Communication': random.choice(['Excellent', 'Good', 'Average']) }\n",
    "        \n",
    "        entry = (name, age, dept, salary, dob, married, gender, langs, contact, coms)\n",
    "        sample_data.append(entry)\n",
    "\n",
    "    return sample_data\n",
    "\n",
    "# Generate sample entries\n",
    "data = generate_sample_data(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a user-specified custom schema\n",
    "\n",
    "# PySpark SQL provides StructType & StructField classes to programmatically specify the structure to the DataFrame.\n",
    "from pyspark.sql.types import StructType,StructField\n",
    "# Class to define the structure of the DataFrame.\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType, BooleanType, DateType, ArrayType, MapType\n",
    "\n",
    "# StructField class to define the columns which include column name(String), column type (DataType), nullable column (Boolean) and metadata (MetaData)\n",
    "\n",
    "# Define the custom schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField('Name', StringType(), True),\n",
    "    StructField('Age', IntegerType(), True),\n",
    "    StructField('Dept', StringType(), True),\n",
    "    StructField('Salary', StringType(), True),\n",
    "    StructField('Birth', StringType(), True),\n",
    "    StructField(\"Married\", BooleanType(), True),\n",
    "    StructField('Gender', StringType(), True),\n",
    "    StructField('Languages', ArrayType(StringType()), True),\n",
    "    StructField('Contact', MapType(StringType(), StringType()), True),\n",
    "    StructField('Skills', MapType(StringType(),StringType()),True)\n",
    "\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "# Show the DataFrame \n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n",
    "# Create df with defined Schema\n",
    "deptDF = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Extract field names from the schema and convert to an array\n",
    "schema_array = [field.name for field in schema.fields]\n",
    "\n",
    "# Create df with defined column names\n",
    "deptDF = spark.createDataFrame(data, schema)\n",
    "deptDF.printSchema()\n",
    "\n",
    "# Assign \n",
    "df = deptDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, concat_ws,  to_json\n",
    "\n",
    "# Save to CSV\n",
    "# CSV format does not support complex types like array or maps directly.\n",
    "# convert the complex fields to json \n",
    "\n",
    "df_csv = df.withColumn(\"Languages\", to_json(\"Languages\")) \\\n",
    "            .withColumn(\"Contact\", to_json(\"Contact\")) \\\n",
    "            .withColumn(\"Skills\", to_json(\"Skills\"))\n",
    "            \n",
    "# PySpark saves data into multiple parts or partitions by default for performance and parallelism reasons. \n",
    "# The number of partitions can be controlled by various configuration options.\n",
    "# To save into single file, repartition to 1. \n",
    "\n",
    "df_single_partition = df_csv.coalesce(1)\n",
    "df_single_partition.write.csv(\"files/csv/person\", header=True, mode=\"overwrite\")\n",
    "\n",
    "df_single_partition = df.coalesce(1)\n",
    "\n",
    "# Save to JSON\n",
    "df_single_partition.write.json(\"files/json/person\",  mode=\"overwrite\", compression=\"gzip\")\n",
    "df_single_partition.write.json(\"files/json/person\",  mode=\"overwrite\")\n",
    "\n",
    "# Save to Parquet\n",
    "df_single_partition.write.parquet(\"files/parquet/person\", mode=\"overwrite\", compression=\"snappy\")\n",
    "\n",
    "# Save to Avro - need to load avro module during pyspark ses\n",
    "df_single_partition.write.format(\"avro\").save(\"files/avro/person\")\n",
    "\n",
    "# Save to ORC\n",
    "df_single_partition.write.orc(\"files/orc/person\", mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV\n",
    "df = spark.read.csv(\"files/csv/person\")\n",
    "# Read using format and load\n",
    "df = spark.read.format(\"csv\").load(\"files/csv/person\")\n",
    "\n",
    "# Read multiple files\n",
    "df = spark.read.csv(\"path1, path2, path3\")\n",
    "\n",
    "# Read all CSV Files in a Directory\n",
    "df = spark.read.csv(\"Folder_Path\")\n",
    "\n",
    "# Read file with Options \n",
    "# PySpark reads all columns as a string (StringType) by default.\n",
    "\n",
    "df3 = spark.read \\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"header\",True) \\\n",
    "    .option(\"inferSchema\",True) \\\n",
    "    .option(delimiter=',') \\\n",
    "    .load(\"files/csv/person\")\n",
    "    \n",
    "# or use options\n",
    "df4 = spark.read.options(header='True', inferSchema='True', delimiter=',') \\\n",
    "            .csv(\"files/csv/person\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write PySpark DataFrame to CSV file\n",
    "df.write.option(\"header\", True) \\\n",
    "    .csv(\"files/csv/person\")\n",
    "\n",
    "df.write.options(header='True', delimiter=',') \\\n",
    "    .csv(\"files/csv/person\")\n",
    "\n",
    "# mode = overwrite / append / ignore (if file exists)\n",
    "df.write.mode('overwrite').csv(\"files/csv/person\")\n",
    "# or use format and save\n",
    "df.write.format(\"csv\").mode('overwrite').save(\"files/csv/person\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSON\n",
    "\n",
    "# Read JSON file into dataframe\n",
    "df = spark.read.json(\"resources/zipcodes.json\")\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n",
    "# Read JSON file into dataframe\n",
    "df = spark.read.format(\"json\") \\\n",
    "        .load(\"resources/zipcodes.json\")\n",
    "\n",
    "# Read multiline json file ( when json is in multiline format )\n",
    "multiline_df = spark.read.option(\"multiline\",\"true\") \\\n",
    "      .json(\"resources/multiline-zipcode.json\")\n",
    "\n",
    "\n",
    "# Read multiple files, from different paths\n",
    "df2 = spark.read.json(\n",
    "    ['folder/file1.json','resources/file2.json'])\n",
    "\n",
    "# Read all JSON files from a folder\n",
    "df3 = spark.read.json(\"resources/*.json\")\n",
    "df3.show()\n",
    "\n",
    "# Reading files with a user-specified custom schema\n",
    "df_with_schema = spark.read.schema(schema) \\\n",
    "        .json(\"resources/zipcodes.json\")\n",
    "df_with_schema.printSchema()\n",
    "\n",
    "# Write PySpark DataFrame to JSON file\n",
    "df2.write.json(\"/tmp/spark_output/zipcodes.json\")\n",
    "\n",
    "# Write with mode = overwrite, append, ignore, errorifexists.\n",
    "df2.write.mode('Overwrite').json(\"/tmp/spark_output/zipcodes.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parquet \n",
    "# Pyspark SQL automatically capture the schema of the original data. \n",
    "# When write a DataFrame to parquet file, it automatically preserves column names and their data types.\n",
    "\n",
    "# Read parquet file using read.parquet()\n",
    "parDF = spark.read.parquet(\"files/parquet/person\")\n",
    "\n",
    "# Write DataFrame to parquet file using write.parquet()\n",
    "df.write.parquet(\"/tmp/output/people.parquet\")\n",
    "# or with mode\n",
    "df.write.mode(\"append\").parquet(\"path/to/parquet/file\")\n",
    "# Using append and overwrite to save parquet file\n",
    "df.write.mode('append').parquet(\"path/to/parquet/file\")\n",
    "df.write.mode('overwrite').parquet(\"path/to/parquet/file\")\n",
    "\n",
    "# Create Parquet partition file\n",
    "df.write.partitionBy(\"gender\",\"salary\").mode(\"overwrite\").parquet(\"/tmp/output/people2.parquet\")\n",
    "# Read parquet with partition gender=M\n",
    "parDF2 = spark.read.parquet(\"/tmp/output/people2.parquet/gender=M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hive\n",
    "# PySpark writes the data to the default Hive warehouse location which is /user/hive/warehouse when you use a Hive cluster.\n",
    "# By default it creates files in parquet format with snappy compression.\n",
    "\n",
    "from os.path import abspath\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# warehouse_location points to the default location for managed databases and tables\n",
    "warehouse_location = abspath('spark-warehouse')\n",
    "\n",
    "# Create spark session with hive enabled\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"read_write_Hive\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "    .config(\"hive.metastore.uris\", \"thrift://remote-host:9083\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create DataFrame \n",
    "data = [(1, \"James\",30,\"M\"), (2, \"Ann\",40,\"F\"),\n",
    "    (3, \"Jeff\",41,\"M\"),(4, \"Jennifer\",20,\"F\")]\n",
    "columns = [\"id\", \"name\",\"age\",\"gender\"]\n",
    "\n",
    "sampleDF = spark.sparkContext.parallelize(data).toDF(columns)\n",
    "\n",
    "# Create Hive Internal table\n",
    "sampleDF.write.mode('overwrite') \\\n",
    "         .saveAsTable(\"employee\")\n",
    "\n",
    "# Save the DataFrame as a Hive table with various options\n",
    "df.write.option(\"compression\", \"gzip\").mode(\"overwrite\").saveAsTable(\"my_hive_table\")\n",
    "\n",
    "# Save the DataFrame as a Hive table with format and additional options\n",
    "df.write.format(\"parquet\").option(\"compression\", \"snappy\").mode(\"append\").saveAsTable(\"my_hive_table_2\")\n",
    "\n",
    "# Read Hive table\n",
    "df = spark.read.table(\"employee\")\n",
    "# or\n",
    "# Read Hive table by SQL\n",
    "df = spark.sql(\"select * from employee\")\n",
    "df.show()\n",
    "\n",
    "# Create Hive Internal table\n",
    "sampleDF.write.mode('overwrite') \\\n",
    "    .saveAsTable(\"emp.employee\")\n",
    "\n",
    "# Create Hive External table\n",
    "sampleDF.write.mode('overwrite')\\\n",
    "        .option(\"path\", \"/path_to_external/table\")\\\n",
    "        .saveAsTable(\"emp.employee\")\n",
    "\n",
    "# Create database \n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS emp\")\n",
    "\n",
    "#  Create table and load data \n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive\")\n",
    "spark.sql(\"LOAD DATA LOCAL INPATH 'path/file.txt' INTO TABLE src\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delta Tables\n",
    "\n",
    "# Delta Lake is an open-source storage layer, providing solution for managing structured and semi-structured data in big data environments. \n",
    "# In Databricks, Delta Tables are tables stored in Delta Lake format.\n",
    "# Delta Tables enable advanced capabilities such as schema enforcement, data versioning, and transactional processing.\n",
    "\n",
    "# Specify the Delta table path\n",
    "delta_table_path = \"/path/to/delta-table\"\n",
    "\n",
    "# Save the DataFrame as a Delta table\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
    "df.write.format(\"delta\").mode(\"append\").save(delta_table_path)\n",
    "\n",
    "# Read data from a Delta table into a DataFrame\n",
    "df_read = spark.read.format(\"delta\").load(delta_table_path)\n",
    "\n",
    "# Update data in a Delta table\n",
    "update_condition = (df_read[\"Name\"] == \"John\")\n",
    "df_read.where(update_condition).update(\"Age\", 30)\n",
    "\n",
    "# Delete data from a Delta table\n",
    "delete_condition = (df_read[\"Name\"] == \"Alice\")\n",
    "df_read.where(delete_condition).delete()\n",
    "\n",
    "# Time Travel in Delta Tables:\n",
    "\n",
    "# List all available versions of a Delta Table\n",
    "df_version = spark.sql(\"DESCRIBE HISTORY delta.`/path/to/delta-table`\")\n",
    "df_version.show()\n",
    "display(df_version)\n",
    "\n",
    "# Display time travel versions for the Delta Table\n",
    "df_history = spark.sql(f\"DESCRIBE HISTORY delta.`{delta_table_path}`\")\n",
    "df_history.show(truncate=False)\n",
    "\n",
    "display(df_history)\n",
    "\n",
    "# Querying/Read data from a Delta Table as of a specific version\n",
    "# Use the option(\"versionAsOf\", version_number) to query the Delta Table at different versions.\n",
    "version_number = 1\n",
    "df_as_of_version = spark.read.format(\"delta\").option(\"versionAsOf\", version_number).load(\"/path/to/delta-table\")\n",
    "df_as_of_version.show()\n",
    "\n",
    "# Schema Evolution in Delta Tables:\n",
    "# Schema evolution allowing to add, drop, or modify columns over time without requiring a full rewrite of the table.\n",
    "# Use option (\"mergeSchema\", True) to enable schema evolution, allowing the Delta table schema to be updated to match the schema of the DataFrame being written.\n",
    "df.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"append\").save(delta_table_path)\n",
    "\n",
    "# Vacuum a Delta table, reclaim storage by removing older versions of data files.\n",
    "# Run the VACUUM operation with optional parameters\n",
    "# RETAIN num HOURS: Specify the minimum number of hours a file must be retained.\n",
    "# RETAIN num VERSIONS: Specify the minimum number of versions a file must be retained.\n",
    "# DRY RUN: Simulates the VACUUM operation without actually deleting files.\n",
    "retention_hours = \"24\"\n",
    "retained_files = spark.sql(f\"VACUUM delta.`{delta_table_path}` RETAIN {retention_hours} HOURS DRY RUN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JDBC is a Java standard to connect to any database as long as you provide the right JDBC connector jar in the classpath and provide a JDBC driver using the JDBC API. \n",
    "# PySpark also leverages the same JDBC standard when using jdbc() method.\n",
    "\n",
    "# To query a database table using jdbc() method, you would need the following.\n",
    "\n",
    "# Server IP or Host name and Port,\n",
    "# Database name,\n",
    "# Table name,\n",
    "# User and Password.\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "           .appName('queryJDBC') \\\n",
    "           .config(\"spark.jars\", \"mysql-connector-java-8.x.xx.jar\") \\\n",
    "           .getOrCreate()\n",
    "\n",
    "# Query table using jdbc()\n",
    "df = spark.read \\\n",
    "    .jdbc(\"jdbc:mysql://localhost:3306/emp\", \"employee\", \\\n",
    "          properties={\"user\": \"root\", \"password\": \"root\", \"driver\":\"com.mysql.cj.jdbc.Driver\"})\n",
    "\n",
    "# show DataFrame\n",
    "df.show()\n",
    "\n",
    "# or by using DataFrameReader.format(\"jdbc\").load()\n",
    "# Query from MySQL Table\n",
    "df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://localhost:3306/emp\") \\\n",
    "    .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .option(\"dbtable\", \"employee\") \\\n",
    "    .option(\"user\", \"root\") \\\n",
    "    .option(\"password\", \"root\") \\\n",
    "    .load()\n",
    "\n",
    "# SQL Query Specific Columns of JDBC Table\n",
    "# above extracts the entire JDBC table into PySpark DataFrame. Sometimes you may be required to query specific columns with where condition\n",
    "# .option(\"query\", \"select id,age from employee where gender='M'\") \\\n",
    "\n",
    "# Read MySQL Table in Parallel\n",
    "# Use option numPartitions to read MySQL table in parallel\n",
    "# Use the fetchsize option which is used to specify how many rows to fetch at a time \n",
    "\n",
    "# Using numPartitions\n",
    "df = spark.read \\\n",
    "  .format(\"jdbc\") \\\n",
    "  .option(\"query\", \"select id,age from employee where gender='M'\") \\\n",
    "  .option(\"numPartitions\",5) \\\n",
    "  .option(\"fetchsize\", 20) \\\n",
    "  .load()\n",
    "\n",
    "# Write to MySQL Table\n",
    "sampleDF.write \\\n",
    "  .format(\"jdbc\") \\\n",
    "  .option(\"driver\",\"com.mysql.cj.jdbc.Driver\") \\\n",
    "  .option(\"url\", \"jdbc:mysql://localhost:3306/emp\") \\\n",
    "  .option(\"dbtable\", \"employee\") \\\n",
    "  .option(\"user\", \"root\") \\\n",
    "  .option(\"password\", \"root\") \\\n",
    "  .save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
