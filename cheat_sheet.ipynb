{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup env varianble, if PySpark used localy.\n",
    "import os\n",
    "os.environ['SPARK_HOME'] = \"/usr/local/spark\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = 'lab'\n",
    "os.environ['PYSPARK_PYTHON'] = 'python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apache Spark is an open-source, distributed computing system that provides an efficient and fast processing engine for large-scale data processing tasks (big data processing) \n",
    "# One of its main characteristics is its ability to perform in-memory processing, reducing the need to read and write to disk and thereby improving overall performance. _\n",
    "# Spark supports various programming languages, including Scala, Java, Python, and R, making it versatile for different use cases. \n",
    "# Its core abstraction is the Resilient Distributed Dataset (RDD), a fault-tolerant collection of elements that can be processed in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkSession which is an entry point to the PySpark application\n",
    "\n",
    "# SparkSession was introduced in version 2.0\n",
    "# Itâ€™s object spark, an entry point to underlying PySpark functionality in order to programmatically create PySpark RDD, DataFrame. \n",
    "# Since 2.0 SparkSession can be used in replace with SQLContext, HiveContext, and other contexts defined prior to 2.0.\n",
    "# SparkSession is a combined class for all different contexts we used to have prior to 2.0 release (SQLContext and HiveContext etc)\n",
    "\n",
    "# You can create MULTIPLE SparkSession objects but only ONE SparkContext per JVM. \n",
    "# In case you want to create another new SparkContext you should stop the existing Sparkcontext using stop() function before creating a new one.\n",
    "\n",
    "# SparkSession internally creates SparkConfig and SparkContext with the configuration provided with SparkSession.\n",
    "\n",
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession, using the builder() method\n",
    "spark = SparkSession.builder \\\n",
    "      .master(\"local[1]\") \\\n",
    "      .appName(\"DataManipulations\") \\\n",
    "      .getOrCreate() \n",
    "\n",
    "# add configs to SparkSession\n",
    "# .config(\"spark.some.config.option\", \"config-value\")  \n",
    "# SparkSession with Hive Enable \n",
    "# .enableHiveSupport()\n",
    "# \n",
    "# https://spark.apache.org/docs/latest/configuration.html\n",
    "\n",
    "\n",
    "# Create Another / New SparkSession\n",
    "# This uses the same app name, master as the existing session. \n",
    "# Underlying SparkContext will be the same for both sessions as you can have only one context per PySpark application.\n",
    "# Many Spark session objects are required when you want to keep Spark tables (relational)\n",
    "\n",
    "spark2 = SparkSession.newSession\n",
    "print(spark2)\n",
    "\n",
    "# Get Existing SparkSession\n",
    "spark3 = SparkSession.builder.getOrCreate()\n",
    "print(spark3)\n",
    "\n",
    "# Once the SparkSession is created, you can add the spark configs during runtime or get all configs.\n",
    "\n",
    "# Set Config\n",
    "spark.conf.set(\"spark.executor.memory\", \"5g\")\n",
    "\n",
    "# Get a Spark Config\n",
    "partitions = spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "print(partitions)\n",
    "\n",
    "# Config information\n",
    "# https://spark.apache.org/docs/latest/configuration.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with Catalogs\n",
    "# To get the catalog metadata, PySpark Session exposes catalog variable. \n",
    "# Note that these methods spark.catalog.listDatabases and spark.catalog.listTables and returns the DataSet.\n",
    "\n",
    "# Get metadata from the Catalog\n",
    "# List databases\n",
    "dbs = spark.catalog.listDatabases()\n",
    "print(dbs)\n",
    "\n",
    "# List Tables\n",
    "tbls = spark.catalog.listTables()\n",
    "print(tbls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Example SparkSession in Apache Spark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MySparkApplication\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create your first data frame\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "df.show()\n",
    "\n",
    "# Create SparkContext in Apache Spark version 2.x and later\n",
    "# Get the SparkContext from the SparkSession\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Check if the SparkSession is active\n",
    "if spark.isActive():\n",
    "    print(\"Spark session is active. Closing it.\")\n",
    "\n",
    "# Get the SparkContext associated with the SparkSession\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Check if the SparkContext is active\n",
    "if not sc.isStopped:\n",
    "    print(\"Spark session is active.\")\n",
    "    # Perform your Spark operations here\n",
    "\n",
    "    # Stop the Spark session\n",
    "    spark.stop()\n",
    "    print(\"Spark session is stopped.\")\n",
    "\n",
    "# To stop SparkSession in Apache Spark, this ensures that resources are properly released and the Spark application terminates correctly\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Empty RDD / DataFrame manually ( with or without schema (column names-datatypes) )\n",
    "\n",
    "# Create Empty RDD with no partition   \n",
    "emptyRDD = spark.sparkContext.emptyRDD()\n",
    "\n",
    "# Creates Empty RDD using parallelize\n",
    "rddEmpty= spark.sparkContext.parallelize([])\n",
    "\n",
    "# Create empty RDD with 10 partitions\n",
    "rdd = spark.sparkContext.parallelize([],10) \n",
    "print(\"Initial partition count:\" + str(rdd.getNumPartitions()))\n",
    "\n",
    "# If you try to perform operations on empty RDD you going to get ValueError(\"RDD is empty\")\n",
    "\n",
    "# Create Empty DataFrame with Schema (StructType)\n",
    "\n",
    "# Create Schema\n",
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "schema = StructType([\n",
    "  StructField('firstname', StringType(), True),\n",
    "  StructField('middlename', StringType(), True),\n",
    "  StructField('lastname', StringType(), True)\n",
    "  ])\n",
    "\n",
    "# Create empty DataFrame from empty RDD\n",
    "df = spark.createDataFrame(emptyRDD, schema)\n",
    "\n",
    "# Convert empty RDD to Dataframe\n",
    "dfConvert = emptyRDD.toDF(schema)\n",
    "\n",
    "# Create empty DataFrame directly.\n",
    "dfEmpty = spark.createDataFrame([], schema)\n",
    "\n",
    "# Create empty DatFrame with no schema (no columns)\n",
    "df3 = spark.createDataFrame([], StructType([]))\n",
    "df3.printSchema()\n",
    "\n",
    "dept = [(\"Finance\",10),(\"Marketing\",20),(\"Sales\",30),(\"IT\",40)]\n",
    "rdd = spark.sparkContext.parallelize(dept)\n",
    "\n",
    "# Convert PySpark RDD to DataFrame\n",
    "dfFromRDD1 = rdd.toDF()\n",
    "\n",
    "# Converts RDD to DataFrame with column names\n",
    "dfFromRDD2 = rdd.toDF(\"col1\",\"col2\")\n",
    "\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "df2 = rdd.toDF(deptColumns)\n",
    "df2.show()\n",
    "\n",
    "# using createDataFrame() - Convert DataFrame to RDD\n",
    "df = spark.createDataFrame(rdd).toDF(\"col1\",\"col2\")\n",
    "\n",
    "# Convert DataFrame to RDD\n",
    "rdd = df.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repartition and Coalesce\n",
    "reparRdd = rdd.repartition(4)\n",
    "\n",
    "# DataFrame coalesce\n",
    "reparRdd = rdd.coalesce(2) \n",
    "print(reparRdd.rdd.getNumPartitions())\n",
    "\n",
    "\n",
    "# RDD Cache - saves RDD computation to storage level `MEMORY_ONLY` meaning it will store the data in the JVM heap\n",
    "cachedRdd = rdd.cache()\n",
    "\n",
    "# RDD Persist - used to store the RDD to one of the storage levels\n",
    "# MEMORY_ONLY,MEMORY_AND_DISK, MEMORY_ONLY_SER, MEMORY_AND_DISK_SER, DISK_ONLY, MEMORY_ONLY_2,MEMORY_AND_DISK_2 \n",
    "\n",
    "import pyspark\n",
    "dfPersist = rdd.persist(pyspark.StorageLevel.MEMORY_ONLY)\n",
    "dfPersist.show(False)\n",
    "\n",
    "# RDD Unpersist - unpersist() marks the RDD as non-persistent, and remove all blocks for it from memory and disk.\n",
    "rddPersist2 = dfPersist.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since Spark 2.0 SparkSession has become an entry point to PySpark to work with RDD, and DataFrame. \n",
    "# Prior to 2.0, SparkContext and RDD used to be an entry point\n",
    "\n",
    "# DataFrame creation\n",
    "# The simplest way to create a DataFrame is from a Python list of data. \n",
    "# DataFrame can also be created from an RDD and by reading files from several sources.\n",
    "\n",
    "# Create a simple DataFrame from List\n",
    "data = [(\"John\", True , 25), (\"Alice\", False , 30), (\"Bob\", True , 22)]\n",
    "columns = [\"Name\", \"Married\", \"Age\"]\n",
    "\n",
    "# Create DataFrame from List\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "\n",
    "# Create DataFrame from Array of Tuples => RDD => DF\n",
    "df = spark.createDataFrame(data).toDF(columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the contents of the DataFrame, display DataFrame in a tabular form\n",
    "df.show()\n",
    "df.show(2, truncate=4) \n",
    "df.show(3, vertical=True) \n",
    "\n",
    "# Assuming you are using Databricks notebook\n",
    "display(df)\n",
    "\n",
    "# Print the schema of the DataFrame, showing the data types of each column.\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to access Columns / Rows\n",
    "# Accessing column by \"name\" or dot .Name\n",
    "df.select(df.Age).show()\n",
    "df.select(df[\"Age\"]).show()\n",
    "\n",
    "# Accessing column using SQL col() function\n",
    "from pyspark.sql.functions import col\n",
    "df.select(col(\"Age\")).show()\n",
    "\n",
    "# Create DataFrame with Nested Struct using Row class\n",
    "from pyspark.sql import Row\n",
    "data = [Row(Name = \"Greg\", Prop = Row(Hair = \"black\", eye = \"blue\")),\n",
    "      Row(Name = \"Ann\", Prop = Row(Hair = \"grey\", eye = \"black\"))]\n",
    "df = spark.createDataFrame(data)\n",
    "df.printSchema()\n",
    "\n",
    "# Access struct column\n",
    "df.select(df.Prop.Hair).show()\n",
    "df.select(df[\"Prop.Hair\"]).show()\n",
    "df.select(col(\"Prop.Hair\")).show()\n",
    "\n",
    "# Access all columns from struct\n",
    "df.select(col(\"Prop.*\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column Class \n",
    "# Column class is a fundamental building block for expressing transformations on DataFrames. \n",
    "# It represents a column expression that can be applied to manipulate or transform data within a DataFrame. \n",
    "# The Column class provides a wide range of methods and functions that you can use to perform operations on the data in a column.\n",
    "\n",
    "# Create a Column instance by referencing a column in a DataFrame using square bracket notation (df['columnName']).\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark Column Operators\n",
    "\n",
    "data=[(100,10,1),(200,20,2),(300,30,3)]\n",
    "df = spark.createDataFrame(data).toDF(\"col1\",\"col2\",\"col3\")\n",
    "df.show()\n",
    "\n",
    "# Arthmetic operations\n",
    "df.select(df.col1 + df.col2).show()\n",
    "df.select(df.col1 - df.col2).show() \n",
    "df.select(df.col1 * df.col2).show()\n",
    "df.select(df.col1 / df.col2).show()\n",
    "df.select(df.col1 % df.col2).show()\n",
    "\n",
    "df.select(df.col2 > df.col3).show()\n",
    "df.select(df.col2 < df.col3).show()\n",
    "df.select(df.col2 == df.col3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some random data, multi-column with diffrent data type, to ilustrate data frame oprations and functions.\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from faker import Faker\n",
    "fake = Faker()\n",
    "\n",
    "def generate_sample_data(num_entries=5):\n",
    "    names = [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eva\"]\n",
    "    departments = [\"HR\", \"IT\", \"Finance\", \"Marketing\", \"Operations\"]\n",
    "\n",
    "    current_year = datetime.now().year\n",
    "    date_of_births = [datetime(current_year - random.randint(10, 50), random.randint(1, 12), random.randint(1, 28)) for _ in range(num_entries)]\n",
    "\n",
    "    sample_data = []\n",
    "\n",
    "    for _ in range(num_entries):\n",
    "        name = fake.first_name()\n",
    "        age = current_year - date_of_births[_].year\n",
    "        dept = random.choice(departments)\n",
    "        salary = random.uniform(40000, 120000)\n",
    "        dob = date_of_births[_].strftime(\"%Y-%m-%d\")\n",
    "        married = random.choice([True, False])\n",
    "        gender = random.choice([\"M\", \"F\", None])\n",
    "        # array of languages spoken\n",
    "        langs = random.sample([\"English\", \"Spanish\", \"French\", \"German\", \"Chinese\"], random.randint(2, 3))\n",
    "        # contact information (dictionary)\n",
    "        contact = {'Phone': fake.phone_number(), 'Email': fake.email()}\n",
    "        # skills (dictionary)\n",
    "        coms = {'Communication': random.choice(['Excellent', 'Good', 'Average']) }\n",
    "        \n",
    "        entry = (name, age, dept, salary, dob, married, gender, langs, contact, coms)\n",
    "        sample_data.append(entry)\n",
    "\n",
    "    return sample_data\n",
    "\n",
    "# Generate sample entries\n",
    "data = generate_sample_data(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to define the structure of the DataFrame.\n",
    "# StructField class to define the columns which include column name(String), column type (DataType), nullable column (Boolean) and metadata (MetaData)\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, BooleanType, DateType, ArrayType, MapType\n",
    "\n",
    "# Define the schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField('Name', StringType(), True),\n",
    "    StructField('Age', IntegerType(), True),\n",
    "    StructField('Dept', StringType(), True),\n",
    "    StructField('Salary', StringType(), True),\n",
    "    StructField('Birth', StringType(), True),\n",
    "    StructField(\"Married\", BooleanType(), True),\n",
    "    StructField('Gender', StringType(), True),\n",
    "    StructField('Languages', ArrayType(StringType()), True),\n",
    "    StructField('Contact', MapType(StringType(), StringType()), True),\n",
    "    StructField('Skills', MapType(StringType(),StringType()),True)\n",
    "\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "df.printSchema()\n",
    "\n",
    "# Create df with defined Schema\n",
    "deptDF = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Extract field names from the schema and convert to an array\n",
    "schema_array = [field.name for field in schema.fields]\n",
    "\n",
    "# Create df with defined column names\n",
    "deptDF = spark.createDataFrame(data, schema)\n",
    "deptDF.printSchema()\n",
    "\n",
    "# Assign \n",
    "df = deptDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if a Column Exists in a DataFrame\n",
    "print(df.schema.fieldNames())\n",
    "\n",
    "# Check if a column exists\n",
    "column_to_check = \"Age\"\n",
    "\n",
    "if column_to_check in df.schema.fieldNames():\n",
    "    print(f\"The column '{column_to_check}' exists in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export schema to Json file\n",
    "printSchemaJson = deptDF.schema.json()\n",
    "print(printSchemaJson)\n",
    "\n",
    "# Load schema from json file\n",
    "import json\n",
    "schemaFromJson = StructType.fromJson(json.loads( printSchemaJson) )\n",
    "\n",
    "# Create df with defined Schema\n",
    "deptDF = spark.createDataFrame(data, schemaFromJson)\n",
    "deptDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS\n",
    "# PySpark Column Functions\n",
    "# import functions\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT single, multiple, column by index, column function\n",
    "# Select is a transformation that returns a new DataFrame and holds the columns that are selected\n",
    "\n",
    "df.select(\"Age\").show()\n",
    "df.select(df.Age,df.Name).show()\n",
    "df.select(df[\"Married\"], df[\"Birth\"]).show()\n",
    "\n",
    "# By using col() function\n",
    "from pyspark.sql.functions import col\n",
    "df.select(col(\"Languages\"),col(\"Skills\")).show()\n",
    "\n",
    "# Select columns by regular expression\n",
    "df.select(df.colRegex(\"`^.*name*`\")).show()\n",
    "\n",
    "# Get all columns from DF\n",
    "all_columns = df.columns\n",
    "\n",
    "# Select All columns from List\n",
    "columns = [\"Age\", \"Name\"]\n",
    "df.select(*columns).show()\n",
    "\n",
    "# Select All columns\n",
    "df.select([col for col in df.columns]).show()\n",
    "df.select(\"*\").show()\n",
    "\n",
    "#Selects first 3 columns and show top 5 rows\n",
    "df.select(df.columns[:3]).show(5)\n",
    "\n",
    "# Selects columns 2 to 4 and show top 5 rows\n",
    "df.select(df.columns[2:4]).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALIAS: create an alias for a column. It allows you to rename a column temporarily for a specific operation or query.\n",
    "# Alias the 'Age' column as 'EmployeeAge'\n",
    "df_alias = df.select(df['Age'].alias('EmployeeAge'))\n",
    "\n",
    "# Show the DataFrame with the aliased column\n",
    "df_alias.show()\n",
    "\n",
    "# Another Alias example with expresion\n",
    "df.select(expr(\" Name ||','|| Age\").alias(\"FullName\") ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WithColumn: add a new column or update an existing column in a DataFrame.\n",
    "\n",
    "# Add a new column \"bonus\" with a constant value\n",
    "df = df.withColumn(\"Bonus\", lit(1000))\n",
    "\n",
    "# Add multiple literal columns\n",
    "df = df.withColumn(\"Country\", lit(\"USA\")).withColumn(\"City\", lit(\"NY\"))\n",
    "\n",
    "# Create a Column from an Existing Column\n",
    "df = df.withColumn(\"ExtraBonus\", col(\"Salary\") * 0.3 )\n",
    "\n",
    "# Update The Value of an Existing Column\n",
    "df = df.withColumn(\"Salary\", col(\"Salary\")*0.2)\n",
    "\n",
    "# Change column DataType\n",
    "df = df.withColumn(\"Salary\", col(\"Salary\").cast(\"Integer\"))\n",
    "\n",
    "# Standardize strings (convert to lowercase)\n",
    "standardized_df = df.withColumn(\"City\", lower(df.City))\n",
    "\n",
    "# Adding / Changing struct of the DataFrame using struct()\n",
    "from pyspark.sql.functions import col, struct, when\n",
    "\n",
    "updatedDF = deptDF.withColumn(\"OtherInfo\", \n",
    "                        struct( col(\"Salary\").alias(\"Salary\"),\n",
    "                                when(col(\"Salary\").cast(IntegerType()) < 2000,\"Low\")\n",
    "                                .when(col(\"Salary\").cast(IntegerType()) < 4000,\"Medium\")\n",
    "                                .otherwise(\"High\").alias(\"Salary_Grade\")\n",
    "                        ))\\\n",
    "                    .drop(\"Skills\",\"Salary\")\n",
    "                              \n",
    "updatedDF.printSchema()\n",
    "updatedDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop column(s)\n",
    "df = df.drop(\"City\", \"Skills\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RENAME Column \n",
    "df = df.withColumnRenamed(\"Salary\", \"Income\")\n",
    "\n",
    "# Rename multiple columns\n",
    "df = df.withColumnRenamed(\"Birth\", \"DateOfBirth\").withColumnRenamed(\"Income\",\"Salary_amount\")\n",
    "\n",
    "# Use toDF() to change all column names.\n",
    "newColumns = df.columns\n",
    "df.toDF(*newColumns).printSchema()\n",
    "\n",
    "# Changing a column name on nested data is not straight forward and requires to create a new schema with new DataFrame columns using StructType \n",
    "# and use it using cast function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILTER \n",
    "# Filter the rows from RDD/DataFrame based on the given condition or SQL expression\n",
    "# Both Filter() and Where() functions operate exactly the same\n",
    "\n",
    "# Filter rows where \"age\" is greater than 30\n",
    "df.filter(col(\"Age\") > 30).show()\n",
    "\n",
    "# Using equals condition\n",
    "df.filter(df.Country == \"USA\").show(truncate=False)\n",
    "\n",
    "# not equals condition\n",
    "df.filter(df.Country != \"USA\").show(truncate=False) \n",
    "df.filter( ~(df.Country == \"USA\")).show(truncate=False)\n",
    "\n",
    "# Using SQL Expression\n",
    "df.filter(\"Gender == 'M'\").show()\n",
    "\n",
    "# Not equal\n",
    "df.filter(\"Gender != 'M'\").show()\n",
    "df.filter(\"Gender <> 'M'\").show()\n",
    "\n",
    "# Filter on String Type Column\n",
    "df.filter(col(\"Name\") == \"Alice\").show()\n",
    "# Or \n",
    "df.where(df[\"Name\"] == \"Alice\")\n",
    "\n",
    "# Filter with Multiple Conditions (AND)\n",
    "multiple_conditions = df.filter((col(\"Age\") > 30) & (col(\"Married\") == True))\n",
    "\n",
    "# Filter on Struct Type Column\n",
    "struct_filter = df.filter(col(\"Skills.Python\") == \"Intermediate\")\n",
    "struct_filter.show()\n",
    "\n",
    "# Filter on Array Type Column\n",
    "from pyspark.sql.functions import array_contains\n",
    "df.filter(array_contains(df.Languages,\"French\")).show()\n",
    "\n",
    "# Using startswith\n",
    "df.filter(df.Country.startswith(\"N\")).show()\n",
    "# Using endswith\n",
    "df.filter(df.Country.endswith(\"H\")).show()\n",
    "\n",
    "# Like - SQL LIKE pattern\n",
    "df.filter(df.Name.like(\"%rose%\")).show()\n",
    "\n",
    "# rlike - SQL RLIKE pattern (LIKE with Regex)\n",
    "# This check case insensitive\n",
    "df.filter(df.Name.rlike(\"(?i)^*rose$\")).show()\n",
    "\n",
    "# Between - Filter rows where 'Age' is between 20 and 30 - filter rows based on whether a column's values fall within a specified range.\n",
    "df_between = df.filter(df['Age'].between(20, 30)).show(5)\n",
    "\n",
    "# Handling outliers (filtering rows with specific conditions)\n",
    "filtered_outliers_df = df.filter((df.Age >= 18) & (df.Age <= 65))\n",
    "\n",
    "# Filter rows where 'Name' contains 'on'  -  filter rows based on whether a column contains a specified substring.\n",
    "df_contains = df.filter(df['Name'].contains('on'))\n",
    "\n",
    "# Filter rows based on whether a column's values are null or not null.\n",
    "# Filter rows where 'Dept' is null\n",
    "df_null = df.filter(df['Dept'].isNull()).show()\n",
    "\n",
    "# Filter rows where 'Dept' is not null\n",
    "df_not_null = df.filter(df['Dept'].isNotNull()).show()\n",
    "\n",
    "# Filter rows where 'Name' starts with 'D'\n",
    "df_startswith = df.filter(df['Name'].startswith('A'))\n",
    "\n",
    "# Filter rows where 'Name' ends with 'y'\n",
    "df_endswith = df.filter(df['Name'].endswith('y'))\n",
    "\n",
    "# Filter rows where 'Name' is like 'Da%'\n",
    "df_like = df.filter(df['Name'].like('Da%'))\n",
    "\n",
    "# Filter rows where 'Name' matches the regular expression 'a$'\n",
    "df_rlike = df.filter(df['Name'].rlike('a$'))\n",
    "\n",
    "# Filter rows where 'Dept' is in ['IT', 'HR']\n",
    "df_isin = df.filter(df['Dept'].isin(['IT', 'HR']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Distinct Rows (By Comparing All Columns) \n",
    "# Distinct returns a new DataFrame after removing the duplicate records. \n",
    "distinctDF = df.distinct()\n",
    "print(\"Distinct count: \"+ str(distinctDF.count()) )\n",
    "\n",
    "# Drop duplicate rows explicitly\n",
    "df_no_duplicates = df.dropDuplicates()\n",
    "print(\"Distinct count: \"+ str(df_no_duplicates.count()))\n",
    "\n",
    "# Distinct of Selected Multiple Columns => Drop duplicates on selected columns\n",
    "dropDisDF = df.dropDuplicates([\"Age\",\"Salary\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SORT or OrderBy: \n",
    "# sort DataFrame by ascending or descending order based on single or multiple columns.\n",
    "# By default, it sort/orders by ascending. \n",
    "# There is no significant difference in terms of functionality or sorting capability between these two methods. \n",
    "# Both can be used to sort rows based on one or more columns in ascending or descending order.\n",
    "\n",
    "# Sort the DataFrame by the \"name\" column in ascending order\n",
    "df_sorted = df.sort(\"Name\", ascending=False).show()\n",
    "\n",
    "# Sort multiple columns in the DataFrame\n",
    "df.sort(col(\"Salary\"), col(\"Name\")).show(truncate=False)\n",
    "\n",
    "# Sort DataFrame with asc/desc functions\n",
    "df.sort(df.Salary.asc(), df.Name.desc()).show(truncate=False)\n",
    "\n",
    "# Order the DataFrame by the \"age\" column in descending order\n",
    "df_ordered = df.orderBy(col(\"Age\").desc()).show()\n",
    "\n",
    "# Sort the DataFrame in ascending /descending order based on the 'Age' column\n",
    "df_asc = df.orderBy(df['Age'].asc())\n",
    "df_desc = df.orderBy(df['Age'].desc())\n",
    "\n",
    "# Sort the DataFrame in ascending order based on 'Age', with nulls first\n",
    "df_asc_nulls_first = df.orderBy(df['Age'].asc_nulls_first())\n",
    "\n",
    "# Sort the DataFrame in ascending order based on 'Age', with nulls last\n",
    "df_desc_nulls_last = df.orderBy(df['Age'].desc_nulls_last())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GroupBy\n",
    "\n",
    "from pyspark.sql.functions import mean, min, max, avg, sum\n",
    "\n",
    "# Create new DataFrames using groupBy and aggregation functions\n",
    "df_count = df.groupBy(\"Dept\").count()\n",
    "df_min_salary = df.groupBy(\"Dept\").min(\"Salary\")\n",
    "df_max_salary = df.groupBy(\"Dept\").max(\"Salary\")\n",
    "df_avg_salary = df.groupBy(\"Dept\").avg(\"Salary\")\n",
    "df_mean_salary = df.groupBy(\"Dept\").mean(\"Salary\")\n",
    "\n",
    "# Group by department and calculate aggregate functions\n",
    "grouped_df = df.groupBy(\"Dept\").agg(\n",
    "    mean(\"Salary\").alias(\"Avg_Salary\"),\n",
    "    min(\"Salary\").alias(\"Min_Salary\"),\n",
    "    max(\"Salary\").alias(\"Max_Salary\"),\n",
    "    avg(\"Age\").alias(\"Avg_Age\"),\n",
    "    sum(\"Age\").alias(\"Total_Age\")\n",
    ")\n",
    "# Show the grouped DataFrame\n",
    "grouped_df.show(truncate=False)\n",
    "\n",
    "# GroupBy on multiple columns\n",
    "df.groupBy(\"Dept\", \"Age\").sum(\"Salary\",\"Bonus\").show()\n",
    "\n",
    "# GroupBy on multiple columns and perform aggregation\n",
    "df_grouped = df.groupBy(\"Dept\", \"Age\").agg({\"Salary\": \"avg\", \"Name\": \"count\"})\n",
    "df_grouped.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fillna and Fill: \n",
    "# Replace missing or null values of a DataFrame. Functionally they both perform same.\n",
    "\n",
    "# Fill null values in all columns with 0\n",
    "df_filled = df.fillna(value=0)\n",
    "\n",
    "# Fill null values in the \"age\" column with 0 , only Age column \n",
    "df_filled = df.fillna(0, subset=[\"Age\"])\n",
    "\n",
    "# Fill all null values with a default value (e.g., empty string)\n",
    "df_filled_all = df.fillna(\"\", subset=df.columns)\n",
    "\n",
    "# replace NULL/None values with an empty string or any constant values\n",
    "df.na.fill(\"\").show(2)\n",
    "\n",
    "# Replace column nulls in pipeline\n",
    "df.na.fill(\"unknown\",[\"City\"]) \\\n",
    "    .na.fill(\"ND\",[\"Gender\"]).show()\n",
    "\n",
    "# Specify different replacement values for different columns when using fillna()\n",
    "df.fillna( {\"Gender\": \"NoInfo\", \"Salary\":50000} ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAP transformation that is used to apply the transformation function (lambda) on every element of RDD/DataFrame and returns a new RDD\n",
    "# the output of map transformations would always have the same number of records as input.\n",
    "\n",
    "# DataFrame doesnâ€™t have map() transformation to apply the lambda function, so you need to convert the DataFrame to RDD and apply the map() transformation.\n",
    "\n",
    "# Convert DataFrame to RDD and apply map transformation\n",
    "rdd_result = df.rdd.map(lambda row: (\n",
    "    row['Name'].upper(),\n",
    "    row['Age'] * 2,\n",
    "    row['Dept'].upper(),\n",
    "    row['Salary'] * 1.1,  # Increase salary by 10%\n",
    "    row['Birth'],\n",
    "    row['Married'],\n",
    "    row['Gender'],\n",
    "    ', '.join(row['Languages']),  # Convert list to comma-separated string\n",
    "    ', '.join([f\"{key}: {value}\" for key, value in row['Contact'].items()]),  # Convert map to string\n",
    "    ', '.join([f\"{key}: {value}\" for key, value in row['Skills'].items()])  # Convert map to string\n",
    "))\n",
    "\n",
    "# Convert RDD back to DataFrame\n",
    "columns_transformed = [\"Name\", \"DoubleAge\", \"UpperDept\", \"IncreasedSalary\", \"Birth\", \"Married\", \"Gender\", \"Languages\", \"Contact\", \"Skills\"]\n",
    "df_transformed = rdd_result.toDF(columns_transformed).show(5)\n",
    "\n",
    "print(df.columns)\n",
    "# columns = ['Name', 'Age', 'Dept', 'Salary', 'Birth', 'Married', 'Gender', 'Languages', 'Contact', 'Skills', 'Bonus', 'Country', 'City', 'ExtraBonus', 'UpperName', 'ReducedSalary']\n",
    "\n",
    "# Referring columns by index\n",
    "rdd_by_index = df.rdd.map(lambda x: (x[0], x[6], x[1]))  # Referring to Name, Gender, and Age by index\n",
    "df_by_index = rdd_by_index.toDF([\"Name\", \"Gender\", \"Age\"])\n",
    "df_by_index.show(5)\n",
    "\n",
    "# Referring column names\n",
    "rdd_by_names = df.rdd.map(lambda x: (x[\"Name\"], x[\"Gender\"], x[\"Age\"]))  # Referring to Name, Gender, and Age by names\n",
    "df_by_names = rdd_by_names.toDF([\"Name\", \"Gender\", \"Age\"])\n",
    "df_by_names.show(5)\n",
    "\n",
    "# By calling function\n",
    "def func_to_modify_DF(row):\n",
    "    name = row.Name\n",
    "    gender = row.Gender\n",
    "    age = row.Age * 2\n",
    "    return (name, gender, age)\n",
    "\n",
    "rdd_by_function = df.rdd.map(lambda x: func_to_modify_DF(x))\n",
    "df_by_function = rdd_by_function.toDF([\"Name\", \"Gender\", \"Age\"])\n",
    "df_by_function.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ForEach action in PySpark is used for performing a function on each element of an RDD or DataFrame. \n",
    "# It is often used for side effects, such as printing or saving results.\n",
    "# foreach() function doesnâ€™t return a value instead it executes the input function on each element of an RDD, DataFrame\n",
    "# mainly used to print row, to manipulate accumulators, update Kafka topics, and other external sources.\n",
    "\n",
    "# Print a nice summary for each row\n",
    "def print_summary(row):\n",
    "    print(f\"{row['Name']} is {row['Age']} years old, {'married' if row['Married'] else 'not married'}, speaks {', '.join(row['Languages'])}, and has skills in {', '.join(row['Skills'].keys())}\")\n",
    "\n",
    "df.foreach(lambda row: print_summary(row))\n",
    "\n",
    "# Use foreach to update an accumulator count for married individuals\n",
    "married_accumulator = spark.sparkContext.accumulator(0)\n",
    "\n",
    "def count_married(row):\n",
    "    if row['Married'] == True:\n",
    "        married_accumulator.add(1)\n",
    "\n",
    "df.foreach(lambda row: count_married(row.asDict()))   # row.asDict() to convert the Row object to a Python dictionary\n",
    "# Print the count of married individuals ,  # Accessed by driver\n",
    "print(f\"Number of married individuals: {married_accumulator.value}\")\n",
    "\n",
    "# Function to produce each row to Kafka\n",
    "def produce_to_kafka(row):\n",
    "    # Assuming you have a Kafka topic named 'employee_topic'\n",
    "    kafka_bootstrap_servers = 'your_kafka_bootstrap_servers'\n",
    "    kafka_topic = 'employee_topic'\n",
    "    kafka_producer_config = {\n",
    "        'bootstrap.servers': kafka_bootstrap_servers,\n",
    "        'key.serializer': 'org.apache.kafka.common.serialization.StringSerializer',\n",
    "        'value.serializer': 'org.apache.kafka.common.serialization.StringSerializer',\n",
    "    }\n",
    "    \n",
    "    # Convert row to JSON string\n",
    "    row_json = row.toJSON().collect()[0]\n",
    "    \n",
    "    # Use Kafka producer to send the row to the topic\n",
    "    from kafka import KafkaProducer\n",
    "    producer = KafkaProducer(**kafka_producer_config)\n",
    "    producer.send(kafka_topic, key=None, value=row_json)\n",
    "    producer.close()\n",
    "\n",
    "# Apply the function using foreach\n",
    "df.foreach(produce_to_kafka)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Function to Column\n",
    "\n",
    "# withColumn(), sql(), select() you can apply a built-in function or custom function to a column. \n",
    "# In order to apply a custom function, first you need to create a function and register the function as a UDF\n",
    "\n",
    "from pyspark.sql.functions import upper\n",
    "\n",
    "# Apply function using withColumn\n",
    "df = df.withColumn(\"UpperName\", upper(df[\"Name\"]))\n",
    "\n",
    "# Apply upper function using select on specific columns\n",
    "df_upper_select = df.select(\n",
    "    upper(col(\"Name\")).alias(\"UpperName_Select\"),\n",
    "    upper(col(\"Dept\")).alias(\"UpperDept_Select\"),\n",
    "    upper(col(\"Gender\")).alias(\"UpperGender_Select\"),\n",
    "    \"*\",\n",
    ")\n",
    "df_upper_select.show(truncate=False)\n",
    "\n",
    "# Custom transformation function\n",
    "def reduce_salary(df, reduceBy):\n",
    "    return df.withColumn(\"ReducedSalary\", df[\"Salary\"].cast(\"float\") - reduceBy)\n",
    "\n",
    "# Apply custom transformation\n",
    "df = reduce_salary(df, reduceBy=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform()\n",
    "# Used to chain the custom transformations and this function returns the new DataFrame after applying the specified transformations.\n",
    "\n",
    "# Custom transformation 1: Convert \"Name\" column to uppercase\n",
    "def to_upper_str_columns(df):\n",
    "    return df.withColumn(\"Name\", upper(df[\"Name\"]))\n",
    "\n",
    "# Custom transformation 2: Reduce salary by a specified amount\n",
    "def reduce_salary(df, reduceBy):\n",
    "    return df.withColumn(\"ReducedSalary\", df[\"Salary\"] - reduceBy)\n",
    "\n",
    "# Custom transformation 3: Apply a discount to the new salary\n",
    "def apply_discount(df, discount):\n",
    "    return df.withColumn(\"DiscountedSalary\", df[\"ReducedSalary\"] - (df[\"ReducedSalary\"] * discount) / 100)\n",
    "\n",
    "# Apply transformations using DataFrame.transform()\n",
    "result_df = df.transform(to_upper_str_columns)\\\n",
    "                .transform(reduce_salary, reduceBy=5000) \\\n",
    "                    .transform(apply_discount, discount = 0.05 )\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "result_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get random sample records from the dataset\n",
    "# Sample 20% of the DataFrame\n",
    "df_sampled = df.sample(withReplacement=False, fraction=0.2, seed=42)\n",
    "\n",
    "# Sample 10% of the DataFrame using \"Name\" column as the sampling key\n",
    "df_sampled_by = df.sampleBy(col=\"Name\", fractions={\"John\": 0.1, \"Alice\": 0.1}, seed=42).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User defined function\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Create a udf function by wrapping the above function with udf().\n",
    "# Convert function to udf\n",
    "\n",
    "# Create custom function\n",
    "def upperCase(str):\n",
    "    return str.upper()\n",
    "\n",
    "upperCaseUDF = udf(lambda x:upperCase(x),StringType()) \n",
    "\n",
    "# Custom UDF with withColumn()\n",
    "df.withColumn(\"Diff Name\", upperCaseUDF(col(\"Name\"))).show(truncate=False)\n",
    "\n",
    "# Define a UDF to convert names to uppercase\n",
    "upper_udf = udf(lambda x: x.upper(), StringType())\n",
    "\n",
    "# Apply the UDF to the \"name\" column\n",
    "df_with_upper_name = df.withColumn(\"upper_name\", upper_udf(\"Name\"))\n",
    "\n",
    "# Custom function to calculate bonus based on age and salary\n",
    "@udf(FloatType())\n",
    "def calculate_bonus(age, salary):\n",
    "    # You can define your custom logic here\n",
    "    # Let's say the bonus is 5% of the salary for employees below 30 and 10% for others\n",
    "    bonus_percentage = 0.05 if age < 30 else 0.1\n",
    "    return salary * bonus_percentage\n",
    "\n",
    "# Apply the custom function using withColumn\n",
    "df_with_bonus = df.withColumn(\"Bonus\", calculate_bonus(df[\"Age\"], df[\"Salary\"]))\n",
    "df_with_bonus.show(truncate=False)\n",
    "\n",
    "# Handling inconsistent data using a custom function\n",
    "def custom_cleaning_function(city):\n",
    "    if city is None:\n",
    "        return \"Unknown\"\n",
    "    city = city.strip()\n",
    "    return city\n",
    "\n",
    "cleaning_udf = udf(custom_cleaning_function, StringType())\n",
    "cleaned_inconsistent_df = df.withColumn(\"City\", cleaning_udf(df.City))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOME functions from SQL\n",
    "# Cast the 'Age' column to StringType\n",
    "df_cast = df.withColumn('Age', df['Age'].cast('string')).printSchema()\n",
    "\n",
    "# Substr: Extract the first three characters of 'Name'\n",
    "df_substr = df.withColumn('SubName', df['Name'].substr(1, 3))\n",
    "\n",
    "# When: Add a new column 'Status' based on the 'Age' column\n",
    "df_status = df.withColumn('Status', when(df['Age'] < 30, 'Young').otherwise('Old'))\n",
    "df.select(df.Name, df.Age, when(df.Dept==\"IT\",\"programista\") \\\n",
    "              .when(df.Dept==\"HR\",\"hr\") \\\n",
    "              .when(df.Dept==None ,\"\") \\\n",
    "              .otherwise(df.Dept)\\\n",
    "              .alias(\"Departmens\") \\\n",
    "    ).show()\n",
    "\n",
    "# PySpark SQL functions lit() and typedLit() are used to add a new column to DataFrame by assigning a literal or constant value \n",
    "# Lit is used to create a new column with a literal value.\n",
    "# Create a new column 'ConstantColumn' with the literal value 42\n",
    "df_lit = df.withColumn('Constant', lit('cos'))\n",
    "\n",
    "# Creates a constant literal column\n",
    "df.withColumn(\"ConstantColumn\", typedLit(\"ConstantValue\"))\n",
    "\n",
    "# Split: used to split a string column into an array of substrings based on a delimiter.\n",
    "# Assume 'Name' is a string column\n",
    "df_split = df.withColumn('NameArray', split('Name', 'a'))\n",
    "# df_split.show()\n",
    "\n",
    "# Explode: used to transform an array or map column into multiple rows, duplicating the values of the other columns.\n",
    "# Assume 'Languages' is an array column\n",
    "df_explode = df.select('Name', 'Age', 'Dept', 'Birth', explode('Languages').alias('Langs'))\n",
    "\n",
    "# Array_contains: used to check if a specified value is present in an array column.\n",
    "# Assume 'Skills' is an array column\n",
    "df_contains = df.filter(array_contains('Languages', 'German'))\n",
    "\n",
    "# Array: used to create an array column.\n",
    "# Create a new array column 'Languages' with values from 'LanguagesSpoken'\n",
    "df_array = df.withColumn('LanguagesSpoken', array('Languages'))\n",
    "\n",
    "# Expr: used to parse a SQL expression and return it as a Column\n",
    "# Create a new column 'TotalAge' as the sum of 'Age' and 5\n",
    "df_expr = df.withColumn('TotalAge', expr('Age + 5'))\n",
    "\n",
    "# Regexp_replace: used to replace occurrences of a specified pattern with a replacement string.\n",
    "# Replace 'o' with '0' in the 'Name' column\n",
    "df_replace = df.withColumn('NameReplaced', regexp_replace('Name', 'o', '0'))\n",
    "\n",
    "# Text cleaning with regex\n",
    "cleaned_text_df = df.withColumn(\"Name\", regexp_replace(df.Name, \"[^a-zA-Z0-9 ]\", \"\"))\n",
    "# df_replace.show()\n",
    "\n",
    "# Creates a struct column\n",
    "df.select(struct(\"Name\", \"Age\", \"Dept\").alias(\"EmployeeInfo\")).show()\n",
    "\n",
    "# Collects the elements of a column into a list => collect_list()\n",
    "df.groupBy(\"Dept\").agg(collect_list(\"Name\").alias(\"EmployeeList\")).show()\n",
    "\n",
    "# Collect_list is used to aggregate values into a list.\n",
    "df_collect_list = df.groupBy('Dept').agg(collect_list('Skills').alias('AllSkills'))\n",
    "# Show the DataFrame with the collected list of skills for each department\n",
    "df_collect_list.show()\n",
    "\n",
    "# Collects the unique elements of a column into a set => collect_set\n",
    "df.groupBy(\"Dept\").agg(collect_set(\"Languages\").alias(\"UniqueLanguages\")).show()\n",
    "df_collect_set = df.groupBy('Dept').agg(collect_set('Contact').alias('UniqueContact'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAP\n",
    "# MapType (also called map type) is a data type to represent Python Dictionary (dict) to store key-value pair\n",
    "\n",
    "# Create MapType From StructType\n",
    "from pyspark.sql.types import StructField, StructType, StringType, MapType\n",
    "schema = StructType([\n",
    "    StructField('Name', StringType(), True),\n",
    "    StructField('Properties', MapType(StringType(),StringType()),True)\n",
    "])\n",
    "\n",
    "dataDictionary = [\n",
    "        ('James',{'hair':'black','eye':'brown'}),\n",
    "        ('Michael',{'hair':'brown','eye':None}),\n",
    "        ]\n",
    "df2 = spark.createDataFrame(data=dataDictionary, schema = schema)\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)\n",
    "\n",
    "# Creates a map from Key-Value pairs. \n",
    "# a map of 'Name','Skills' for each entry\n",
    "df_create_map = df.select(\"Name\", create_map('Name', 'Skills').alias('NameSkillsMap'))\n",
    "# df_create_map.show(truncate=False)\n",
    "\n",
    "# Map_keys:  Extracts the keys from a map column, All Map Keys\n",
    "df.select(\"Name\", map_keys(\"Contact\").alias(\"SkillKeys\")).show()\n",
    "\n",
    "# Map_values: Extracts the values from a map column, All map Values\n",
    "df.select(\"Name\", map_values(\"Skills\")).show()\n",
    "\n",
    "# GetItem is used to extract an element from an array type column.\n",
    "# Assume 'Languages' is an array type column\n",
    "df_item = df.select(df['Languages'].getItem(1).alias('LangSkill'))\n",
    "# Extract values from the \"skills\" map column\n",
    "df2 = df2.withColumn(\"Hairs\", col(\"Properties\").getItem(\"hair\")).show()\n",
    "\n",
    "# GetField is used to extract a field from a struct type column\n",
    "# 'Contact' is a struct type column with fields 'Phone', 'Email', and 'Address'\n",
    "df_field = df.select(df['Contact'].getField('Phone').alias('PhoneNumber'))\n",
    "\n",
    "# Explode data => new row for nested items\n",
    "df2.select(df2.Name, explode(df2.Properties)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partitioning the data on the file system is a way to improve the performance of the query when dealing with a large dataset\n",
    "# PySpark partition is a way to split a large dataset into smaller datasets based on one or more partition keys.\n",
    "# pyspark.sql.DataFrameWriter class which is used to partition the large dataset (DataFrame) into smaller files based on one or multiple columns while writing to disk\n",
    "# When you create a DataFrame from a file/table, based on certain parameters PySpark creates the DataFrame with a certain number of partitions in memory.\n",
    "# PySpark supports partition in two ways; partition in memory (DataFrame) and partition on the disk (File system).\n",
    "\n",
    "# Partition in memory: \n",
    "# You can partition or repartition the DataFrame by calling repartition() or coalesce() transformations.\n",
    "# repartition() creates specified number of partitions in memory. \n",
    "\n",
    "# Partition on disk\n",
    "# Write DataFrame to parquet files partitioned by the \"married\" column\n",
    "df.write.partitionBy(\"Married\").mode(\"overwrite\").parquet(\"output_file_path\")\n",
    "\n",
    "# or CSV writter\n",
    "df.write.option(\"header\", True) \\\n",
    "        .partitionBy(\"Married\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .csv(\"married-state\")\n",
    "\n",
    "# partitionBy() Multiple Columns \n",
    "df.write.partitionBy(\"Country\", \"City\").parquet(\"output_file_path_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JOINS are used to combine two DataFrames and by chaining these you can join multiple DataFrames\n",
    "# It supports all basic join type operations available in traditional SQL like INNER, LEFT OUTER, RIGHT OUTER, LEFT ANTI, LEFT SEMI, CROSS, SELF JOIN\n",
    "\n",
    "# Generate sample data for employees and departments\n",
    "employees_data = [\n",
    "    (1, \"Alice\", 3),\n",
    "    (2, \"Bob\", 1),\n",
    "    (3, \"Charlie\", 2),\n",
    "    (4, \"David\", 2),\n",
    "]\n",
    "\n",
    "departments_data = [\n",
    "    (1, \"IT\"),\n",
    "    (2, \"HR\"),\n",
    "    (3, \"Finance\"),\n",
    "]\n",
    "\n",
    "# Create DataFrames\n",
    "employees_df = spark.createDataFrame(employees_data, ['id', 'name', 'manager'])\n",
    "departments_df = spark.createDataFrame(departments_data, ['id', 'dept'])\n",
    "\n",
    "# Inner Join: Returns only the rows with matching keys in both DataFrames.\n",
    "inner_join = employees_df.join(departments_df, \"id\").show()\n",
    "\n",
    "# Left Join: Returns all rows from the left DataFrame and matching rows from the right DataFrame.\n",
    "left_join = employees_df.join(departments_df, employees_df[\"manager\"] == departments_df[\"id\"], \"left\").show()\n",
    "\n",
    "# Right Join: Returns all rows from the right DataFrame and matching rows from the left DataFrame.\n",
    "right_join = employees_df.join(departments_df, \"id\", \"right\").show()\n",
    "\n",
    "# Full Outer Join: Returns all rows from both DataFrames, including matching and non-matching rows.\n",
    "full_outer = employees_df.join(departments_df, employees_df[\"manager\"] == departments_df[\"id\"], \"outer\").show()\n",
    "\n",
    "# Left Semi Join: Returns all rows from the left DataFrame where there is a match in the right DataFrame.\n",
    "left_semi_join = employees_df.join(departments_df, \"id\", \"left_semi\")\n",
    "\n",
    "# Left Anti Join: Returns all rows from the left DataFrame where there is no match in the right DataFrame.\n",
    "left_anti_join = employees_df.join(departments_df, \"id\", \"left_anti\")\n",
    "\n",
    "# Self Join on Employee (manager = id)\n",
    "self_join = employees_df.alias(\"e1\").join(employees_df.alias(\"e2\"), col(\"e1.manager\") == col(\"e2.id\"), \"inner\").show()\n",
    "\n",
    "# Cross Join (Cartesian Join - all possible combinations of rows)\n",
    "cross_join = employees_df.crossJoin(departments_df)\n",
    "\n",
    "# Join on multiple dataFrames\n",
    "# df1.join(df2,df1.id1 == df2.id2,\"inner\") \\\n",
    "#    .join(df3,df1.id1 == df3.id3,\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joins Using SQL Expression\n",
    "\n",
    "# Create temporary views for your DataFrames\n",
    "employees_df.createOrReplaceTempView(\"EMP\")\n",
    "departments_df.createOrReplaceTempView(\"DEPT\")\n",
    "\n",
    "# Inner Join using SQL Expression\n",
    "result_inner = spark.sql(\"SELECT * FROM EMP INNER JOIN DEPT ON EMP.id = DEPT.id\")\n",
    "\n",
    "# Left Join using SQL Expression\n",
    "result_left = spark.sql(\"SELECT * FROM EMP LEFT JOIN DEPT ON EMP.id = DEPT.id\")\n",
    "\n",
    "# Right Join using SQL Expression\n",
    "result_right = spark.sql(\"SELECT * FROM EMP RIGHT JOIN DEPT ON EMP.id = DEPT.id\")\n",
    "\n",
    "# Full Outer Join using SQL Expression\n",
    "result_outer = spark.sql(\"SELECT * FROM EMP FULL OUTER JOIN DEPT ON EMP.id = DEPT.id\")\n",
    "\n",
    "# Left Semi Join using SQL Expression\n",
    "result_left_semi = spark.sql(\"SELECT * FROM EMP LEFT SEMI JOIN DEPT ON EMP.id = DEPT.id\")\n",
    "\n",
    "# Left Anti Join using SQL Expression\n",
    "result_left_anti = spark.sql(\"SELECT * FROM EMP LEFT ANTI JOIN DEPT ON EMP.id = DEPT.id\")\n",
    "\n",
    "# Cross Join using SQL Expression\n",
    "result_cross = spark.sql(\"SELECT * FROM EMP CROSS JOIN DEPT\")\n",
    "\n",
    "# Self Join on Employee using SQL Expression\n",
    "result_self_join = spark.sql(\"SELECT * FROM EMP e1 INNER JOIN EMP e2 ON e1.manager = e2.id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNION\n",
    "# Union() and UnionAll() transformations are used to merge two or more DataFrameâ€™s of the same schema or structure.\n",
    "\n",
    "# Generate additional sample data for employees_df for demonstration\n",
    "additional_employees_data = [\n",
    "    (5, \"Eva\", 1),\n",
    "    (6, \"Frank\", 3),\n",
    "    (7, \"Grace\", 2),\n",
    "]\n",
    "\n",
    "# Create a new DataFrame with additional sample data\n",
    "additional_employees_df = spark.createDataFrame(additional_employees_data, [\"id\", \"name\", \"manager\"])\n",
    "\n",
    "# Union the two DataFrames\n",
    "combined_employees_df = employees_df.union(additional_employees_df)\n",
    "#combined_employees_df.show()\n",
    "\n",
    "# In PySpark Union does not remove duplicates. Recommend using DataFrame duplicate() function to remove duplicate rows.\n",
    "# Merge without Duplicates\n",
    "disDF = employees_df.union(additional_employees_df).distinct()\n",
    "disDF.show(truncate=False)\n",
    "\n",
    "# Union two DataFrames with different column order\n",
    "# unionByName() is used to merge two DataFrames by column names instead of by position.\n",
    "\n",
    "# Additional sample data with columns in a different order\n",
    "additional_employees_data = [\n",
    "    (5, 1, \"Eva\", \"Female\"),\n",
    "    (6, 3, \"Frank\", \"Male\"),\n",
    "    (7, 2, \"Grace\", \"Female\"),\n",
    "]\n",
    "\n",
    "# Create a new DataFrame with additional sample data and different column order\n",
    "additional_employees_df = spark.createDataFrame(additional_employees_data, [\"id\", \"gender\", \"manager\", \"name\"])\n",
    "\n",
    "# Union the two DataFrames by name\n",
    "# provides an argument allowMissingColumns to specify if you have a different column counts\n",
    "combined_employees_df = employees_df.unionByName(additional_employees_df, allowMissingColumns=True)\n",
    "combined_employees_df.show()\n",
    "\n",
    "# Union two DataFrames with different column order\n",
    "df_union_by_name = df.unionByName(df.select(\"Married\", \"Name\", \"Age\", \"Salary\", \"Languages\", \"Skills\"), allowMissingColumns=True)\n",
    "df_union_by_name.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot is used to rotate/transpose the data from one column into multiple Dataframe columns and back using unpivot(). \n",
    "# Pivot() is an aggregation where one of the grouping columns values is transposed into individual columns with distinct data.\n",
    "\n",
    "# Pivot the DataFrame based on the \"Dept\" column\n",
    "pivot_df = df.groupBy(\"Name\").pivot(\"Dept\").agg({\"Salary\": \"max\"}).show()\n",
    "\n",
    "# Unpivot the \"Languages\" column using explode\n",
    "unpivot_df = df.select(\"Name\", \"Age\", explode(\"Languages\").alias(\"Language\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate Functions\n",
    "\n",
    "# Count the number of rows\n",
    "count_rows = df.count()\n",
    "print(f\"Count of rows: {count_rows}\")\n",
    "\n",
    "# Average age\n",
    "average_age = df.agg(avg(\"Age\").alias(\"AverageAge\"))\n",
    "\n",
    "# Sum of salaries\n",
    "total_salary = df.agg(sum(\"Salary\").alias(\"TotalSalary\"))\n",
    "\n",
    "# Maximum and minimum salary\n",
    "max_salary = df.agg(max(\"Salary\").alias(\"MaxSalary\"))\n",
    "min_salary = df.agg(min(\"Salary\").alias(\"MinSalary\"))\n",
    "\n",
    "# Standard deviation of ages\n",
    "std_dev_age = df.agg(stddev(\"Age\").alias(\"StdDevAge\"))\n",
    "\n",
    "# Variance of ages\n",
    "variance_age = df.agg(variance(\"Age\").alias(\"VarianceAge\"))\n",
    "\n",
    "# first: Returns the first value in a group/column.\n",
    "first_value = df.groupBy(\"Dept\").agg(first(\"Name\").alias(\"FirstEmployee\"))\n",
    "# Last: Returns the last value in a group/column.\n",
    "last_value = df.groupBy(\"Dept\").agg(last(\"Name\").alias(\"LastEmployee\"))\n",
    "\n",
    "# First and last names\n",
    "first_name = df.agg(first(\"Name\").alias(\"FirstName\"))\n",
    "last_name = df.agg(last(\"Name\").alias(\"LastName\"))\n",
    "\n",
    "# Approx_count_distinct: Returns the approximate number of distinct values in a column.\n",
    "approx_distinct_count = df.agg(approx_count_distinct(\"Name\").alias(\"ApproxDistinctCount\"))\n",
    "\n",
    "# Collect_list: Collects the values of a column into a list.\n",
    "collected_list = df.groupBy(\"Dept\").agg(collect_list(\"Name\").alias(\"NameList\"))\n",
    "\n",
    "# Collect_set: Collects the distinct values of a column into a set.\n",
    "collected_set = df.groupBy(\"Dept\").agg(collect_set(\"Gender\").alias(\"GenderSet\"))\n",
    "\n",
    "# CountDistinct: Returns the count of distinct values in a column.\n",
    "distinct_count = df.agg(countDistinct(\"Dept\").alias(\"DistinctDeptCount\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window functions\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, rank, dense_rank, row_number, sum, avg, max, min\n",
    "\n",
    "\n",
    "# PySpark Window functions operate on a group of rows (like frame, partition) and return a single value for every input row.\n",
    "# Window functions are applied over a specific range of rows related to the current row.\n",
    "\n",
    "# Define a Window specification\n",
    "window_spec = Window.orderBy(col(\"Salary\"))\n",
    "\n",
    "# Window specification by department and Order by salary\n",
    "window_spec_2 = Window.partitionBy(\"Dept\").orderBy(\"Salary\")\n",
    "\n",
    "# Rank: used to provide a rank to the result within a window partition.\n",
    "# Rank employees by salary =  assigns a unique rank to each row based on the \"Salary\" column.\n",
    "df_rank = df.withColumn(\"SalaryRank\", rank().over(window_spec))\n",
    "\n",
    "# Dense_rank: used to get the result with rank of rows within a window partition without any gaps.\n",
    "# The dense_rank function is similar to rank but does not leave gaps between ranks when there are tied values.\n",
    "df_dense_rank = df.withColumn(\"SalaryDenseRank\", dense_rank().over(window_spec))\n",
    "\n",
    "# Row_number:  assigns a unique number to each row based on the order specified in the window.\n",
    "df_row_number = df.withColumn(\"SalaryRowNumber\", row_number().over(window_spec)).show()\n",
    "\n",
    "# Calculate the cumulative sum of salaries\n",
    "df_cumulative_sum = df.withColumn(\"CumulativeSalary\", sum(col(\"Salary\")).over(window_spec))\n",
    "\n",
    "# Calculate the average salary over a rolling window\n",
    "df_avg_salary = df.withColumn(\"RollingAvgSalary\", avg(col(\"Salary\")).over(window_spec))\n",
    "\n",
    "# Get the maximum salary for each department\n",
    "df_max_salary_per_dept = df.withColumn(\"MaxSalaryPerDept\", max(col(\"Salary\")).over(Window.partitionBy(\"Dept\")))\n",
    "\n",
    "# Get the minimum salary for each department\n",
    "df_min_salary_per_dept = df.withColumn(\"MinSalaryPerDept\", min(col(\"Salary\")).over(Window.partitionBy(\"Dept\")))\n",
    "\n",
    "# Lead: returns the value of a given expression for a row at a specified physical offset from that row within the result set.\n",
    "df_lead = df.withColumn(\"NextSalary\", lead(\"Salary\").over(window_spec_2))\n",
    "\n",
    "# Lag: returns the value of a given expression for a row at a specified physical offset from that row within the result set.\n",
    "df_lag = df.withColumn(\"PrevSalary\", lag(\"Salary\").over(window_spec_2))\n",
    "\n",
    "# Cume Dist: returns the cumulative distribution of a value within a partition of the result set.\n",
    "df_cume_dist = df.withColumn(\"CumulativeDistribution\", cume_dist().over(window_spec_2))\n",
    "\n",
    "# Ntile: returns the relative rank of a value within a partition of the result set.\n",
    "df_ntile = df.withColumn(\"Ntile\", ntile(4).over(window_spec_2))  # Here, 4 represents the number of tiles.\n",
    "\n",
    "# Percent Rank: returns the relative rank of a value within a partition of the result set, ranging from 0 to 1.\n",
    "df_percent_rank = df.withColumn(\"PercentRank\", percent_rank().over(window_spec_2))\n",
    "\n",
    "# Aggregations for each department using window functions\n",
    "\n",
    "# Avg, Sum, Min, Max, Median, Std for each department\n",
    "df_aggregations = df.withColumn(\"AvgSalary\", avg(\"Salary\").over(window_spec)) \\\n",
    "    .withColumn(\"SumSalary\", sum(\"Salary\").over(window_spec)) \\\n",
    "    .withColumn(\"MinSalary\", min(\"Salary\").over(window_spec)) \\\n",
    "    .withColumn(\"MaxSalary\", max(\"Salary\").over(window_spec)) \\\n",
    "    .withColumn(\"MedianSalary\", expr(\"percentile_approx(Salary, 0.5)\").over(window_spec)) \\\n",
    "    .withColumn(\"StdSalary\", stddev(\"Salary\").over(window_spec))\n",
    "\n",
    "# Show the DataFrame with added columns for aggregations\n",
    "df_aggregations.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date and Timestamp Functions\n",
    "# DateType default format is yyyy-MM-dd \n",
    "# TimestampType default format is yyyy-MM-dd HH:mm:ss.SSSS\n",
    "\n",
    "# Extracting Year, Month, Day from \"Birth\" column\n",
    "df_with_date_parts = df.withColumn(\"Birth_Year\", col(\"Birth\").substr(1, 4).cast(IntegerType())) \\\n",
    "                       .withColumn(\"Birth_Month\", col(\"Birth\").substr(6, 2).cast(IntegerType())) \\\n",
    "                       .withColumn(\"Birth_Day\", col(\"Birth\").substr(9, 2).cast(IntegerType()))\n",
    "\n",
    "# Convert the \"Birth\" column to a DateType\n",
    "df = df.withColumn(\"Birth\", to_date(col(\"Birth\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# Convert a string column to a date.\n",
    "df = df.withColumn('BirthDate', to_date('Birth', 'yyyy-MM-dd'))\n",
    "\n",
    "# Current date and timestamp\n",
    "df_with_current_date = df_with_date_parts.withColumn(\"CurrentDate\", current_date())\n",
    "df_with_current_timestamp = df_with_current_date.withColumn(\"CurrentTimestamp\", current_timestamp())\n",
    "\n",
    "# Adding and subtracting days from \"Birth\" column\n",
    "df_with_added_days = df_with_current_timestamp.withColumn(\"BirthPlus10Days\", date_add(col(\"Birth\"), 10))\n",
    "df_with_subtracted_days = df_with_added_days.withColumn(\"BirthMinus5Days\", date_sub(col(\"Birth\"), 5))\n",
    "\n",
    "# Calculating the difference in days between two dates\n",
    "df_with_date_diff = df_with_subtracted_days.withColumn(\"DateDiff\", datediff(col(\"Birth\"), col(\"BirthMinus5Days\")))\n",
    "\n",
    "# Converting string to date\n",
    "df_with_date_conversion = df_with_date_diff.withColumn(\"BirthAsDate\", to_date(col(\"Birth\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# Converting timestamp to UTC\n",
    "df_with_utc_timestamp = df_with_date_conversion.withColumn(\"UTC_Timestamp\", to_utc_timestamp(col(\"CurrentTimestamp\"), \"GMT\"))\n",
    "\n",
    "# Converting timestamp to Unix timestamp\n",
    "df_with_unix_timestamp = df_with_utc_timestamp.withColumn(\"Unix_Timestamp\", unix_timestamp(col(\"CurrentTimestamp\")))\n",
    "\n",
    "# date_format is used to format a date or timestamp column.\n",
    "# Assume 'BirthDate' is a date column\n",
    "df_date_format = df.withColumn('FormattedDate', date_format('BirthDate', 'MM/dd/yyyy'))\n",
    "\n",
    "# Formatting Date\n",
    "df = df.withColumn(\"FormattedBirth\", date_format(\"Birth\", \"MMM dd, yyyy\"))\n",
    "\n",
    "# Calculate Age using datediff\n",
    "df = df.withColumn(\"CurrentDate\", trunc(current_date(), \"yyyy-MM-dd\"))\n",
    "\n",
    "df = df.withColumn(\"AgeCalculated\", datediff(\"CurrentDate\", \"Birth\") / 365)\n",
    "\n",
    "# Datediff is used to calculate the difference in days between two date columns.\n",
    "# Assume 'BirthDate' and 'CurrentDate' are date columns\n",
    "df_datediff = df.withColumn('DaysSinceBirth', datediff(current_date(), 'BirthDate'))\n",
    "\n",
    "# Assume 'BirthDate' and 'CurrentDate' are date columns\n",
    "df_months_between = df.withColumn('MonthsSinceBirth', months_between(current_date(), 'BirthDate'))\n",
    "\n",
    "# Day of the week\n",
    "df = df.withColumn(\"NextMonday\", next_day(\"Birth\", \"Mon\"))\n",
    "\n",
    "# Week of the year\n",
    "df = df.withColumn(\"WeekOfYear\", weekofyear(\"Birth\"))\n",
    "\n",
    "# Day of the week\n",
    "df = df.withColumn(\"DayOfWeek\", dayofweek(\"Birth\"))\n",
    "\n",
    "# Day of the month\n",
    "df = df.withColumn(\"DayOfMonth\", dayofmonth(\"Birth\"))\n",
    "\n",
    "# Day of the year\n",
    "df = df.withColumn(\"DayOfYear\", dayofyear(\"Birth\"))\n",
    "\n",
    "# Convert string to timestamp, \n",
    "# Assume 'Birth' is a string column in 'yyyy-MM-dd' format\n",
    "df = df.withColumn(\"BirthTimestamp\", to_timestamp(\"Birth\", \"yyyy-MM-dd\"))\n",
    "\n",
    "# Extract Hour, Minute, and Second from timestamp\n",
    "df = df.withColumn(\"Hour\", hour(\"BirthTimestamp\"))\n",
    "df = df.withColumn(\"Minute\", minute(\"BirthTimestamp\"))\n",
    "df = df.withColumn(\"Second\", second(\"BirthTimestamp\"))\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "df.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON \n",
    "# Parses a JSON string column into a struct or map.\n",
    "df.select(\"Name\", from_json(\"Contact\", schema_of_json(\"Contact\")).alias(\"ContactInfo\"))\n",
    "\n",
    "# Converts a struct or map column to a JSON string.\n",
    "df.select(\"Name\", to_json(\"Contact\").alias(\"ContactJSON\"))\n",
    "\n",
    "# Extracts values from a JSON string column using specified keys.\n",
    "df.select(\"Name\", json_tuple(\"Contact\", \"Phone\", \"Email\").alias(\"ContactDetails\"))\n",
    "\n",
    "# Extracts a value from a JSON string column using a JSON path expression.\n",
    "df.select(\"Name\", get_json_object(\"Contact\", \"$.Email\").alias(\"EmailAddress\"))\n",
    "\n",
    "#Infers the schema of a JSON string column.\n",
    "df.select(\"Name\", schema_of_json(\"Contact\").alias(\"ContactSchema\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PySpark Dataframe to Pandas DataFrame\n",
    "pandasDF = deptDF.toPandas()\n",
    "print(pandasDF)\n",
    "\n",
    "# toPandas() results in the collection of all records in the PySpark DataFrame to the driver program and should be done only on a small subset of the data\n",
    "# Pandas API on Spark > 3.2\n",
    "\n",
    "# DataFrame corresponds to pandas DataFrame logically and it holds Spark DataFrame internally. \n",
    "# In other words, it is a wrapper class for Spark DataFrame to behave similarly to pandas DataFrame.\n",
    "\n",
    "# Import PySpark Pandas\n",
    "import pyspark.pandas as ps\n",
    "\n",
    "# Create pandas DataFrame\n",
    "technologies   = ({\n",
    "    'Courses':[\"Spark\",\"PySpark\",\"Hadoop\",\"Python\",\"Pandas\",\"Hadoop\",\"Spark\",\"Python\",\"NA\"],\n",
    "    'Fee' :[22000,25000,23000,24000,26000,25000,25000,22000,1500],\n",
    "    'Duration':['30days','50days','55days','40days','60days','35days','30days','50days','40days'],\n",
    "    'Discount':[1000,2300,1000,1200,2500,None,1400,1600,0]\n",
    "          })\n",
    "df = ps.DataFrame(technologies)\n",
    "print(df)\n",
    "\n",
    "# Use groupby() to compute the sum\n",
    "df2 = df.groupby(['Courses']).sum()\n",
    "print(df2)\n",
    "\n",
    "# Convert this pyspark.pandas.frame.DataFrame object to pandas.core.frame.DataFrame (Convert Pandas API on Spark to Pandas DataFrame)\n",
    "pdf = df.to_pandas()\n",
    "print(type(pdf))\n",
    "\n",
    "# Convert pandas.core.frame.DataFrame to pyspark.pandas.frame.DataFrame (Convert Pandas DataFrame to Pandas API on Spark DataFrame)\n",
    "psdf = ps.from_pandas(pdf)\n",
    "print(type(psdf))\n",
    "\n",
    "# Pandas API on Spark Dataframe into a Spark DataFrame\n",
    "# Converts object from type pyspark.pandas.frame.DataFrame to pyspark.sql.dataframe.DataFrame.\n",
    "sdf = df.to_spark()\n",
    "print(type(sdf))\n",
    "sdf.show()\n",
    "\n",
    "# Convert a Spark Dataframe into a Pandas API on Spark Dataframe\n",
    "# Convert pyspark.sql.dataframe.DataFrame to pyspark.pandas.frame.DataFrame DataFrame\n",
    "psdf = sdf.pandas_api()\n",
    "print(type(psdf))\n",
    "# (or)\n",
    "# to_pandas_on_spark() is depricated\n",
    "psdf = sdf.to_pandas_on_spark()\n",
    "print(type(psdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark SQL brings native RAW SQL queries on Spark meaning you can run traditional ANSI SQL on Spark Dataframe \n",
    "\n",
    "# Using SparkSession you can access PySpark/Spark SQL capabilities in PySpark. \n",
    "# Allow to run any traditional SQL queries on DataFrame using PySpark SQL.\n",
    "\n",
    "# In order to use SQL features first, you need to create a temporary view in PySpark. \n",
    "# Once you have a temporary view you can run any ANSI SQL queries using spark.sql() met\n",
    "\n",
    "# Spark SQL\n",
    "df.createOrReplaceTempView(\"PERSON_DATA\")\n",
    "df2 = spark.sql(\"SELECT * from PERSON_DATA\")\n",
    "df2.printSchema()\n",
    "df2.show()\n",
    "\n",
    "# Query dataframe by SQL\n",
    "groupDF = spark.sql(\"SELECT gender, count(*) from PERSON_DATA group by gender\")\n",
    "groupDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to have a temporary view that is shared among all sessions and keep alive until the Spark application terminates, \n",
    "# you can create a global temporary view using createGlobalTempView()\n",
    "\n",
    "# In PySpark, you can use the createGlobalTempView method to create a global temporary view of a DataFrame. \n",
    "# A global temporary view is accessible from any SparkSession within the same Spark application, and it's useful when you want to share data across different SparkSessions or notebooks.\n",
    "\n",
    "# In this use case, you create a global temporary view, and then in another SparkSession, you can access and query the same global temporary view by specifying the view name as global_temp.<view_name>.\n",
    "\n",
    "# Create a global temporary view,  Assuming you have a DataFrame named 'df'\n",
    "df.createGlobalTempView(\"global_sample_table\")\n",
    "\n",
    "# In another SparkSession or notebook, you can access the global temporary view\n",
    "df2 = spark.newSession().sql(\"SELECT _1, _2 FROM global_temp.global_sample_table\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkSession is used to create and query Hive tables. Note that in order to do this for testing you donâ€™t need Hive to be installed. \n",
    "# saveAsTable() creates Hive managed table. Query the table using spark.sql().\n",
    "\n",
    "# Save the contents of the global temporary view to a persistent table\n",
    "spark.sql(\"SELECT * FROM global_temp.global_sample_table\").write.saveAsTable(\"my_persistent_table\")\n",
    "\n",
    "# Create Hive table & query it.  \n",
    "spark.table(\"sample_table\").write.saveAsTable(\"sample_hive_table\")\n",
    "df3 = spark.sql(\"SELECT _1,_2 FROM sample_hive_table\")\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Database CT\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS ct\")\n",
    "\n",
    "# Create a Table naming as sampleTable under CT database.\n",
    "spark.sql(\"CREATE TABLE ct.sampleTable (id Int, name String, age Int, gender String)\")\n",
    "\n",
    "# Insert into sampleTable using the sampleView. \n",
    "spark.sql(\"INSERT INTO TABLE ct.sampleTable  SELECT * FROM sampleView\")\n",
    "\n",
    "# Lets view the data in the table\n",
    "spark.sql(\"SELECT * FROM ct.sampleTable\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark Streaming\n",
    "# Streaming is a scalable, high-throughput, fault-tolerant streaming processing system that supports both batch and streaming workloads. It is used to process real-time data from sources like file system folders, TCP sockets, S3, Kafka, Flume, Twitter, and Amazon Kinesis to name a few. \n",
    "# The processed data can be pushed to databases, Kafka, live dashboards e.t.c\n",
    "\n",
    "# Streaming from socket\n",
    "df = spark.readStream \\\n",
    "      .format(\"socket\") \\\n",
    "      .option(\"host\",\"localhost\") \\\n",
    "      .option(\"port\",\"9090\") \\\n",
    "      .load()\n",
    "\n",
    "# Streaming from Kafka\n",
    "df = spark.readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"192.168.1.100:9092\") \\\n",
    "        .option(\"subscribe\", \"json_topic\") \\\n",
    "        .option(\"startingOffsets\", \"earliest\") \\\n",
    "        .load()\n",
    "\n",
    "#  Write a message to another topic in Kafka using writeStream()\n",
    "\n",
    "df.selectExpr(\"CAST(id AS STRING) AS key\", \"to_json(struct(*)) AS value\") \\\n",
    "   .writeStream \\\n",
    "   .format(\"kafka\") \\\n",
    "   .outputMode(\"append\") \\\n",
    "   .option(\"kafka.bootstrap.servers\", \"192.168.1.100:9092\") \\\n",
    "   .option(\"topic\", \"josn_data_topic\") \\\n",
    "   .start() \\\n",
    "   .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Spark RDD, which stands for Resilient Distributed Dataset, is a fundamental data structure in Apache Spark, \n",
    "a powerful framework for big data processing. \n",
    "In simple words, you can think of an RDD as a distributed collection of data that can be processed in parallel across multiple computers.\n",
    "\n",
    "Imagine you have a massive dataset (like a huge text file or a database) that is too big to fit on a single computer. \n",
    "An RDD helps you break that data into smaller, manageable chunks and distribute those chunks across a cluster of computers (nodes). \n",
    "Each computer in the cluster can process its own chunk of data independently.\n",
    "\n",
    "For example, if you have a text file containing information about sales transactions, you can create an RDD from it. \n",
    "Each line in the file would become a separate element in the RDD. \n",
    "The RDD is divided into partitions, and each partition is processed on a different computer in the cluster.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" \n",
    "RDDs (Resilient Distributed Datasets) are a core concept in Apache Spark, a powerful distributed data processing framework. \n",
    "RDDs provide a flexible and fault-tolerant way to process and analyze large datasets in parallel across a cluster of computers. \n",
    "Here are some key aspects of RDDs:\n",
    "\n",
    "- Distributed Data: RDDs represent distributed collections of data, split into smaller partitions, and distributed across multiple nodes in a Spark cluster. \n",
    "Each node processes its portion of the data in parallel, making it suitable for big data processing.\n",
    "\n",
    "- Resilience: The \"Resilient\" part of RDDs means they are fault-tolerant. If a node fails during processing, \n",
    "Spark can recompute the lost data using the lineage information (a record of the transformations applied to the data).\n",
    "This ensures data consistency and reliability.\n",
    "\n",
    "- Immutable: RDDs are immutable, meaning their content cannot be changed once created. \n",
    "You can transform an RDD into another RDD, but you cannot modify it in place. \n",
    "This immutability simplifies distributed data processing because it eliminates the need for complex synchronization mechanisms.\n",
    "\n",
    "- Lazy Evaluation: RDDs follow a lazy evaluation model. When you perform operations on an RDD, the transformations are not immediately executed. \n",
    "Instead, they are recorded as a sequence of transformations (a lineage). Actual computation occurs only when an action is called. \n",
    "This allows Spark to optimize the execution plan.\n",
    "\n",
    "- Resilient Transformations: RDDs support two types of operations:\n",
    "\n",
    "a) Transformations: These are operations that create a new RDD from an existing one, such as map, filter, reduceByKey, and join. \n",
    "Transformations are lazily evaluated.\n",
    "b) Actions: These are operations that trigger the execution of transformations and return values, such as count, collect, and saveAsTextFile.\n",
    "\n",
    "RDDs have been a foundational concept in Spark, but it's important to note that as of Spark 2.0 and later, \n",
    "DataFrames and Datasets have become more prevalent for structured data processing, offering higher-level abstractions and optimizations. \n",
    "Nonetheless, RDDs are still useful for more fine-grained control over data processing and remain a crucial part of the Spark ecosystem.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" \n",
    "Data lineage information in the context of distributed data processing, like in Apache Spark, refers to a record or history of how your data was transformed from its original state to its current form. It traces the sequence of operations or transformations that were applied to the data.\n",
    "\n",
    "Think of data lineage like a family tree or genealogy chart for your data. It shows the parent-child relationships between different versions of the data, indicating which operations were performed on the data to produce each new version. This lineage information is essential for maintaining data integrity, \n",
    "fault tolerance, and ensuring that computations can be re-executed if there's a failure.\n",
    "\n",
    "In simple words, data lineage answers the question, \"How did we get this data?\" It's like a step-by-step history that shows how your data evolved and was derived from its original source through various transformations and operations. \n",
    "This information is crucial for debugging, auditing, and ensuring that you can recover or reproduce your data and analysis if something goes wrong.\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
