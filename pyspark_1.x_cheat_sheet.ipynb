{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pyspark.SparkContext is an entry point to the PySpark functionality that is used to communicate with the cluster and to create an RDD, accumulator, and broadcast variables. \n",
    "# Note that you can create only one SparkContext per JVM, in order to create another first you need to stop the existing one using stop() method.\n",
    "# At any given time only one SparkContext instance should be active per JVM. In case you want to create another you should stop existing SparkContext using stop() before creating a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SparkContext in Apache Spark version 1.x (prior to 2.0) - SparkContext is low level interacton.\n",
    "\n",
    "# Import required modules\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "# Create a SparkContext object\n",
    "sc = SparkContext(\"local\", \"MySparkApplication\")\n",
    "print(sc.appName)\n",
    "\n",
    "# Get active SparkContext information\n",
    "sc\n",
    "\n",
    "# Create a Spark configuration with custom parameters\n",
    "# Set the master to run locally with as many threads as available\n",
    "# Set the amount of memory allocated per executor\n",
    "# Set the amount of memory allocated to the driver\n",
    "# Set the number of cores used by each executor\n",
    "# Set the default number of partitions\n",
    "\n",
    "conf = SparkConf().setAppName(\"AppName\") \\\n",
    "                   .setMaster(\"local[*]\") \\\n",
    "                   .set(\"spark.executor.memory\", \"2g\") \\\n",
    "                   .set(\"spark.driver.memory\", \"1g\") \\\n",
    "                   .set(\"spark.executor.cores\", \"2\") \\\n",
    "                   .set(\"spark.default.parallelism\", \"4\")  \n",
    "\n",
    "\n",
    "# Create a Spark context\n",
    "sc = SparkContext(conf=conf)\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "print(sc.appName)\n",
    "\n",
    "# Get the application ID\n",
    "app_id = sc.applicationId\n",
    "print(\"Application ID:\", app_id)\n",
    "\n",
    "# Get the PySpark version\n",
    "spark_version = sc.version\n",
    "print(\"PySpark version:\", spark_version)\n",
    "\n",
    "# Get the URL of the Spark Web UI\n",
    "web_ui_url = sc.uiWebUrl\n",
    "print(\"Spark Web UI URL:\", web_ui_url)\n",
    "\n",
    "# Stop the SparkContext\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some sample data for RDD processing.\n",
    "\n",
    "import random\n",
    "\n",
    "# Function to generate random sales data\n",
    "def generate_sales_data():\n",
    "    products = [\"ProductA\", \"ProductB\", \"ProductC\"]\n",
    "    locations = [\"Location1\", \"Location2\", \"Location3\"]\n",
    "    sales = random.randint(1, 1000)\n",
    "    product = random.choice(products)\n",
    "    location = random.choice(locations)\n",
    "    return f\"{product},{location},{sales}\"\n",
    "\n",
    "# Number of records to generate\n",
    "num_records = 100\n",
    "\n",
    "# Generate and save the sales data\n",
    "with open(\"sample_data.txt\", \"w\") as file:\n",
    "    for _ in range(num_records):\n",
    "        sales_record = generate_sales_data()\n",
    "        file.write(sales_record + \"\\n\")\n",
    "\n",
    "print(f\"{num_records} sales records generated and saved to sales_data.txt.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RDD from a range (0-1000, every 5)\n",
    "rdd_range = sc.parallelize(range(0, 1001, 5))\n",
    "\n",
    "# Create an RDD from a distributed collection (list in this case)\n",
    "data_list = [10, 20, 30, 40, 50]\n",
    "rdd_from_list = sc.parallelize(data_list)\n",
    "\n",
    "# Create an RDD from a list of objects (id, name, age)\n",
    "data_objects = [\n",
    "    {\"id\": 1, \"name\": \"John\", \"age\": 25},\n",
    "    {\"id\": 2, \"name\": \"Jane\", \"age\": 30},\n",
    "    {\"id\": 3, \"name\": \"Bob\", \"age\": 22},\n",
    "]\n",
    "rdd_objects = sc.parallelize(data_objects)\n",
    "\n",
    "# Create an RDD from a text file\n",
    "sales_rdd = sc.textFile(\"sample_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once you have an RDD, you can perform transformation and action operations. Any operation you perform on RDD runs in parallel.\n",
    "\n",
    "# Now, you can perform operations on the RDD, like filtering, mapping, and reducing\n",
    "total_sales = sales_rdd \\\n",
    "    .filter(lambda line: \"ProductA\" in line) \\\n",
    "    .map(lambda line: float(line.split(\",\")[1])) \\\n",
    "    .reduce(lambda x, y: x + y)\n",
    "\n",
    "print(\"Total sales of ProductA: \", total_sales)\n",
    "\n",
    "# On PySpark RDD, you can perform two kinds of operations.\n",
    "\n",
    "# RDD transformations – Transformations are lazy operations. When you run a transformation (for example update), \n",
    "# instead of updating a current RDD, these operations return another RDD.\n",
    "# Lazy meaning they don’t execute until you call an action on RDD. \n",
    "# Some transformations on RDDs are flatMap(), map(), reduceByKey(), filter(), sortByKey() and return a new RDD instead of updating the current.\n",
    "\n",
    "# RDD Action operations return the values from an RDD to a driver program. In other words, any RDD function that returns non-RDD is considered as an action. \n",
    "# RDD Actions – operations that trigger computation and return RDD values to the driver.\n",
    "# RDD Action operation returns the values from an RDD to a driver node. \n",
    "# In other words, any RDD function that returns non RDD[T] is considered as an action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations examples\n",
    "\n",
    "# Map transformation to extract product names\n",
    "product_names_rdd = sales_rdd.map(lambda line: line.split(',')[0])\n",
    "\n",
    "# Filter transformation to get sales greater than 500\n",
    "high_sales_rdd = sales_rdd.filter(lambda line: int(line.split(',')[2]) > 500)\n",
    "\n",
    "# FlatMap transformation to flatten the product names\n",
    "flat_mapped_rdd = product_names_rdd.flatMap(lambda name: name)\n",
    "\n",
    "# Union transformation to combine two RDDs\n",
    "combined_rdd = product_names_rdd.union(high_sales_rdd.map(lambda line: line.split(',')[0]))\n",
    "\n",
    "# GroupBy transformation to group data by product\n",
    "grouped_rdd = sales_rdd.groupBy(lambda line: line.split(',')[0])\n",
    "\n",
    "# Sample transformation to take a random sample of the data\n",
    "sample_rdd = sales_rdd.sample(False, 0.2)  # 20% of the data\n",
    "\n",
    "# Distinct transformation to get unique products\n",
    "distinct_products_rdd = product_names_rdd.distinct()\n",
    "\n",
    "# KeyBy transformation to create key-value pairs with product as the key\n",
    "key_value_rdd = product_names_rdd.keyBy(lambda name: name[0])\n",
    "\n",
    "# Cartesian transformation to find the Cartesian product with another RDD\n",
    "other_rdd = sc.parallelize([(1, \"Info1\"), (2, \"Info2\")])\n",
    "cartesian_result = sales_rdd.cartesian(other_rdd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actions examples\n",
    "\n",
    "# Collect action to retrieve all elements\n",
    "all_elements = sales_rdd.collect()\n",
    "\n",
    "# Count action to count the number of records\n",
    "record_count = sales_rdd.count()\n",
    "\n",
    "# First action to get the first element\n",
    "first_element = sales_rdd.first()\n",
    "\n",
    "# Take action to get a specified number of elements\n",
    "sample_elements = sales_rdd.take(5)\n",
    "\n",
    "# Reduce action to calculate the total sales\n",
    "total_sales = sales_rdd.map(lambda line: int(line.split(',')[2])).reduce(lambda x, y: x + y)\n",
    "\n",
    "# ForEach action to print each element\n",
    "sales_rdd.foreach(lambda line: print(line))\n",
    "\n",
    "# SaveAsTextFile action to save the RDD to a text file\n",
    "sales_rdd.saveAsTextFile(\"output_directory\")\n",
    "\n",
    "# Stop the Spark context\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repartition transformation to change the number of partitions\n",
    "repartitioned_rdd = sales_rdd.repartition(4)  # Change the number of partitions to 4\n",
    "\n",
    "# Coalesce transformation to decrease the number of partitions\n",
    "coalesced_rdd = repartitioned_rdd.coalesce(2)  # Decrease the number of partitions to 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcast transformation to efficiently distribute a read-only variable to all nodes\n",
    "broadcast_variable = sc.broadcast([\"Location1\", \"Location2\", \"Location3\"])\n",
    "filtered_broadcast_rdd = sales_rdd.filter(lambda line: line.split(',')[1] in broadcast_variable.value)\n",
    "\n",
    "# Accumulator to accumulate values across multiple tasks\n",
    "# Initialize an accumulator with an initial value of 0\n",
    "accumulator = sc.accumulator(0)\n",
    "\n",
    "sales_rdd.foreach(lambda line: accumulator.add(int(line.split(',')[2])))\n",
    "total_sales_acc = accumulator.value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
